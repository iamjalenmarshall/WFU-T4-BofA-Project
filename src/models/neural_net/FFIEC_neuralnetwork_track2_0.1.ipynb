{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d83cb0a-9673-41a9-9166-e7af1b7d7eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FFIEC NEURAL NETWORK ANOMALY DETECTION (Track 2 - Per-Bank)\n",
      "Shallow Autoencoder + LOF Ensemble, Trained Individually\n",
      "======================================================================\n",
      "\n",
      "Found 6 per-bank QoQ files in per_bank_qoq/:\n",
      "  ffiec_bank_of_america_qoq.csv\n",
      "  ffiec_citibank_qoq.csv\n",
      "  ffiec_goldman_sachs_bank_usa_qoq.csv\n",
      "  ffiec_jpmorgan_chase_bank_qoq.csv\n",
      "  ffiec_morgan_stanley_bank_qoq.csv\n",
      "  ffiec_wells_fargo_bank_qoq.csv\n",
      "\n",
      "======================================================================\n",
      "  BANK OF AMERICA\n",
      "======================================================================\n",
      "  Loaded: 430 rows x 99 columns\n",
      "  Detected transposed format (features as rows). Transposing...\n",
      "  After transpose: 98 rows x 431 columns\n",
      "  QoQ features: 430\n",
      "  Quarters: 98\n",
      "  Obs/feature ratio: 0.23\n",
      "\n",
      "  [1/2] Training Shallow Autoencoder...\n",
      "    Architecture: (16, 8, 16) (8-dim bottleneck)\n",
      "    L2 regularization: 0.05\n",
      "    PCA: 95% variance retention\n",
      "    PCA components: 10\n",
      "    Training iterations: 454\n",
      "    Final loss: 190824540.598304\n",
      "    NN anomalies: 5\n",
      "\n",
      "  [2/2] Running Local Outlier Factor...\n",
      "    LOF anomalies: 5\n",
      "\n",
      "  Combining scores (NN: 60%, LOF: 40%)...\n",
      "  Ensemble anomalies: 5\n",
      "  Flagged by BOTH models: 3 (high confidence)\n",
      "\n",
      "  Saved: ffiec_bank_of_america_nn_anomalies.csv (98 rows)\n",
      "  Saved: ffiec_bank_of_america_nn_anomalies_flagged.csv (5 rows)\n",
      "  Saved: nn_report_bank_of_america.txt\n",
      "\n",
      "======================================================================\n",
      "  CITIBANK\n",
      "======================================================================\n",
      "  Loaded: 421 rows x 99 columns\n",
      "  Detected transposed format (features as rows). Transposing...\n",
      "  After transpose: 98 rows x 422 columns\n",
      "  QoQ features: 421\n",
      "  Quarters: 98\n",
      "  Obs/feature ratio: 0.23\n",
      "\n",
      "  [1/2] Training Shallow Autoencoder...\n",
      "    Architecture: (16, 8, 16) (8-dim bottleneck)\n",
      "    L2 regularization: 0.05\n",
      "    PCA: 95% variance retention\n",
      "    PCA components: 10\n",
      "    Training iterations: 204\n",
      "    Final loss: 9687555.287637\n",
      "    NN anomalies: 5\n",
      "\n",
      "  [2/2] Running Local Outlier Factor...\n",
      "    LOF anomalies: 5\n",
      "\n",
      "  Combining scores (NN: 60%, LOF: 40%)...\n",
      "  Ensemble anomalies: 5\n",
      "  Flagged by BOTH models: 4 (high confidence)\n",
      "\n",
      "  Saved: ffiec_citibank_nn_anomalies.csv (98 rows)\n",
      "  Saved: ffiec_citibank_nn_anomalies_flagged.csv (5 rows)\n",
      "  Saved: nn_report_citibank.txt\n",
      "\n",
      "======================================================================\n",
      "  GOLDMAN SACHS BANK USA\n",
      "======================================================================\n",
      "  Loaded: 189 rows x 68 columns\n",
      "  Detected transposed format (features as rows). Transposing...\n",
      "  After transpose: 67 rows x 190 columns\n",
      "  QoQ features: 189\n",
      "  Quarters: 67\n",
      "  Obs/feature ratio: 0.35\n",
      "\n",
      "  [1/2] Training Shallow Autoencoder...\n",
      "    Architecture: (16, 8, 16) (8-dim bottleneck)\n",
      "    L2 regularization: 0.05\n",
      "    PCA: 95% variance retention\n",
      "    Validation fraction reduced to 15% (small sample)\n",
      "    PCA components: 10\n",
      "    Training iterations: 180\n",
      "    Final loss: 977427.389977\n",
      "    NN anomalies: 4\n",
      "\n",
      "  [2/2] Running Local Outlier Factor...\n",
      "    LOF anomalies: 4\n",
      "\n",
      "  Combining scores (NN: 60%, LOF: 40%)...\n",
      "  Ensemble anomalies: 4\n",
      "  Flagged by BOTH models: 2 (high confidence)\n",
      "\n",
      "  Saved: ffiec_goldman_sachs_bank_usa_nn_anomalies.csv (67 rows)\n",
      "  Saved: ffiec_goldman_sachs_bank_usa_nn_anomalies_flagged.csv (4 rows)\n",
      "  Saved: nn_report_goldman_sachs_bank_usa.txt\n",
      "\n",
      "======================================================================\n",
      "  JPMORGAN CHASE BANK\n",
      "======================================================================\n",
      "  Loaded: 404 rows x 99 columns\n",
      "  Detected transposed format (features as rows). Transposing...\n",
      "  After transpose: 98 rows x 405 columns\n",
      "  QoQ features: 404\n",
      "  Quarters: 98\n",
      "  Obs/feature ratio: 0.24\n",
      "\n",
      "  [1/2] Training Shallow Autoencoder...\n",
      "    Architecture: (16, 8, 16) (8-dim bottleneck)\n",
      "    L2 regularization: 0.05\n",
      "    PCA: 95% variance retention\n",
      "    PCA components: 10\n",
      "    Training iterations: 172\n",
      "    Final loss: 4548695.896602\n",
      "    NN anomalies: 5\n",
      "\n",
      "  [2/2] Running Local Outlier Factor...\n",
      "    LOF anomalies: 5\n",
      "\n",
      "  Combining scores (NN: 60%, LOF: 40%)...\n",
      "  Ensemble anomalies: 5\n",
      "  Flagged by BOTH models: 3 (high confidence)\n",
      "\n",
      "  Saved: ffiec_jpmorgan_chase_bank_nn_anomalies.csv (98 rows)\n",
      "  Saved: ffiec_jpmorgan_chase_bank_nn_anomalies_flagged.csv (5 rows)\n",
      "  Saved: nn_report_jpmorgan_chase_bank.txt\n",
      "\n",
      "======================================================================\n",
      "  MORGAN STANLEY BANK\n",
      "======================================================================\n",
      "  Loaded: 87 rows x 99 columns\n",
      "  Detected transposed format (features as rows). Transposing...\n",
      "  After transpose: 98 rows x 88 columns\n",
      "  QoQ features: 87\n",
      "  Quarters: 98\n",
      "  Obs/feature ratio: 1.13\n",
      "\n",
      "  [1/2] Training Shallow Autoencoder...\n",
      "    Architecture: (16, 8, 16) (8-dim bottleneck)\n",
      "    L2 regularization: 0.05\n",
      "    PCA: 95% variance retention\n",
      "    PCA components: 10\n",
      "    Training iterations: 227\n",
      "    Final loss: 214069.558341\n",
      "    NN anomalies: 5\n",
      "\n",
      "  [2/2] Running Local Outlier Factor...\n",
      "    LOF anomalies: 5\n",
      "\n",
      "  Combining scores (NN: 60%, LOF: 40%)...\n",
      "  Ensemble anomalies: 5\n",
      "  Flagged by BOTH models: 4 (high confidence)\n",
      "\n",
      "  Saved: ffiec_morgan_stanley_bank_nn_anomalies.csv (98 rows)\n",
      "  Saved: ffiec_morgan_stanley_bank_nn_anomalies_flagged.csv (5 rows)\n",
      "  Saved: nn_report_morgan_stanley_bank.txt\n",
      "\n",
      "======================================================================\n",
      "  WELLS FARGO BANK\n",
      "======================================================================\n",
      "  Loaded: 409 rows x 99 columns\n",
      "  Detected transposed format (features as rows). Transposing...\n",
      "  After transpose: 98 rows x 410 columns\n",
      "  QoQ features: 409\n",
      "  Quarters: 98\n",
      "  Obs/feature ratio: 0.24\n",
      "\n",
      "  [1/2] Training Shallow Autoencoder...\n",
      "    Architecture: (16, 8, 16) (8-dim bottleneck)\n",
      "    L2 regularization: 0.05\n",
      "    PCA: 95% variance retention\n",
      "    PCA components: 10\n",
      "    Training iterations: 192\n",
      "    Final loss: 28213647.714368\n",
      "    NN anomalies: 5\n",
      "\n",
      "  [2/2] Running Local Outlier Factor...\n",
      "    LOF anomalies: 5\n",
      "\n",
      "  Combining scores (NN: 60%, LOF: 40%)...\n",
      "  Ensemble anomalies: 5\n",
      "  Flagged by BOTH models: 3 (high confidence)\n",
      "\n",
      "  Saved: ffiec_wells_fargo_bank_nn_anomalies.csv (98 rows)\n",
      "  Saved: ffiec_wells_fargo_bank_nn_anomalies_flagged.csv (5 rows)\n",
      "  Saved: nn_report_wells_fargo_bank.txt\n",
      "\n",
      "\n",
      "======================================================================\n",
      "CROSS-BANK SUMMARY\n",
      "======================================================================\n",
      "Saved: nn_cross_bank_summary.txt\n",
      "\n",
      "Bank                             Obs  Feat   PCA  Anom  MaxScore\n",
      "----------------------------------------------------------------------\n",
      "Bank of America                   98   430    10     5     100.0\n",
      "Citibank                          98   421    10     5     100.0\n",
      "Goldman Sachs Bank USA            67   189    10     4     100.0\n",
      "JPMorgan Chase Bank               98   404    10     5     100.0\n",
      "Morgan Stanley Bank               98    87    10     5     100.0\n",
      "Wells Fargo Bank                  98   409    10     5     100.0\n",
      "\n",
      "TOP 10 ANOMALIES ACROSS ALL BANKS:\n",
      "  100.0: Bank of America - 12/31/2007\n",
      "  100.0: Citibank - 12/31/2008\n",
      "  100.0: JPMorgan Chase Bank - 12/31/2020\n",
      "  100.0: Wells Fargo Bank - 06/30/2017\n",
      "  100.0: Goldman Sachs Bank USA - 06/30/2016\n",
      "  100.0: Morgan Stanley Bank - 03/31/2006\n",
      "  27.2: Morgan Stanley Bank - 09/30/2001\n",
      "  13.3: Wells Fargo Bank - 06/30/2012\n",
      "  11.3: Morgan Stanley Bank - 03/31/2003\n",
      "  11.2: Wells Fargo Bank - 03/31/2004\n",
      "\n",
      "======================================================================\n",
      "COMPLETE\n",
      "All outputs saved to C:\\Users\\olive\\per_bank_nn/\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FFIEC Neural Network Anomaly Detection (Step 5b Track 2 - Per-Bank)\n",
    "====================================================================\n",
    "Purpose: Detect anomalous quarters for each bank INDIVIDUALLY using\n",
    "         a neural network autoencoder + LOF ensemble, trained on that\n",
    "         bank's own historical QoQ changes.\n",
    "\n",
    "Track 2 Philosophy:\n",
    "  - Each bank gets its OWN model trained only on its own history\n",
    "  - \"Anomalous\" = this quarter is unusual for THIS bank specifically\n",
    "  - Contrast with Track 1 where anomalous = unusual vs. all 6 banks\n",
    "\n",
    "Architecture adapted for small per-bank samples (~93 obs each):\n",
    "  - Shallower bottleneck: Input -> 16 -> 8 -> 16 -> Output\n",
    "  - Heavier L2 regularization (alpha=0.05)\n",
    "  - More aggressive PCA (85% variance retention)\n",
    "  - Larger validation fraction (20%)\n",
    "  - Reduced LOF neighbors (12)\n",
    "\n",
    "Input:  per_bank_qoq/ffiec_<bank_name>_qoq.csv  (from Step 4 Track 2)\n",
    "Output: per_bank_nn/ffiec_<bank_name>_nn_anomalies.csv       (all obs w/ scores)\n",
    "        per_bank_nn/ffiec_<bank_name>_nn_anomalies_flagged.csv (flagged only)\n",
    "        per_bank_nn/nn_report_<bank_name>.txt                 (per-bank report)\n",
    "        per_bank_nn/nn_cross_bank_summary.txt                 (comparison report)\n",
    "\n",
    "Author: Wake Forest MSBA Practicum Team 4\n",
    "Date: February 2026\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "INPUT_DIR = Path(\"per_bank_qoq\")\n",
    "OUTPUT_DIR = Path(\"per_bank_nn\")\n",
    "\n",
    "# Column identifiers\n",
    "QUARTER_COLUMN = \"quarter\"\n",
    "\n",
    "# Neural Network Architecture — adapted for ~93 obs per bank\n",
    "# Smaller bottleneck to prevent memorization with tiny samples\n",
    "HIDDEN_LAYERS = (16, 8, 16)\n",
    "ALPHA = 0.05              # Heavier L2 reg than Track 1 (was 0.01)\n",
    "MAX_ITER = 500\n",
    "LEARNING_RATE = 0.001\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# PCA — balance between compression and information retention\n",
    "PCA_VARIANCE = 0.95       # Retain 95% variance (was 0.80, which collapsed to 1 component)\n",
    "PCA_MIN_COMPONENTS = 10   # Always keep at least 10 components regardless of variance\n",
    "\n",
    "# LOF — fewer neighbors for small samples\n",
    "LOF_NEIGHBORS = 12        # Was 20 in Track 1 (20/93 = 21% is too much)\n",
    "LOF_CONTAMINATION = 0.05\n",
    "\n",
    "# Anomaly Detection\n",
    "CONTAMINATION = 0.05      # Flag top 5% as anomalies\n",
    "\n",
    "# Ensemble weights\n",
    "NN_WEIGHT = 0.6\n",
    "LOF_WEIGHT = 0.4\n",
    "\n",
    "# Bank name lookup (IDRSSD -> display name)\n",
    "BANKS = {\n",
    "    480228:  'Bank of America',\n",
    "    852218:  'JPMorgan Chase Bank',\n",
    "    451965:  'Wells Fargo Bank',\n",
    "    476810:  'Citibank',\n",
    "    1456501: 'Morgan Stanley Bank',\n",
    "    2182786: 'Goldman Sachs Bank USA',\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# AUTOENCODER CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class ShallowAutoencoder:\n",
    "    \"\"\"\n",
    "    Shallow autoencoder for per-bank anomaly detection.\n",
    "\n",
    "    Adapted for small sample sizes (~93 obs):\n",
    "      - 8-dim bottleneck (vs 32 in Track 1)\n",
    "      - Heavier regularization\n",
    "      - Larger validation fraction\n",
    "      - More aggressive PCA preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_layers=(16, 8, 16), alpha=0.05,\n",
    "                 max_iter=500, learning_rate=0.001, random_state=42,\n",
    "                 use_pca=True, pca_variance=0.95, pca_min_components=10):\n",
    "\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.alpha = alpha\n",
    "        self.use_pca = use_pca\n",
    "        self.pca_variance = pca_variance\n",
    "        self.pca_min_components = pca_min_components\n",
    "\n",
    "        self.scaler = RobustScaler()\n",
    "        self.pca = None  # Will be created in fit() after determining n_components\n",
    "\n",
    "        self.model = MLPRegressor(\n",
    "            hidden_layer_sizes=hidden_layers,\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=alpha,\n",
    "            learning_rate='adaptive',\n",
    "            learning_rate_init=learning_rate,\n",
    "            max_iter=max_iter,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.20,   # Larger than Track 1's 0.15\n",
    "            n_iter_no_change=30,\n",
    "            random_state=random_state,\n",
    "            verbose=False\n",
    "        )\n",
    "        self.is_fitted = False\n",
    "        self.n_components_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Train autoencoder with optional PCA preprocessing.\"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        if self.use_pca:\n",
    "            # First pass: determine how many components for the target variance\n",
    "            pca_probe = PCA(n_components=self.pca_variance, random_state=42)\n",
    "            pca_probe.fit(X_scaled)\n",
    "            n_variance_components = pca_probe.n_components_\n",
    "\n",
    "            # Apply minimum floor, but don't exceed sample size or feature count\n",
    "            max_possible = min(X_scaled.shape[0], X_scaled.shape[1])\n",
    "            n_components = max(n_variance_components, self.pca_min_components)\n",
    "            n_components = min(n_components, max_possible - 1)  # Leave room for validation\n",
    "\n",
    "            self.pca = PCA(n_components=n_components, random_state=42)\n",
    "            X_transformed = self.pca.fit_transform(X_scaled)\n",
    "            self.n_components_ = X_transformed.shape[1]\n",
    "        else:\n",
    "            X_transformed = X_scaled\n",
    "            self.n_components_ = X_transformed.shape[1]\n",
    "\n",
    "        # Train: reconstruct input from itself\n",
    "        self.model.fit(X_transformed, X_transformed)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data through scaling and optional PCA.\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        if self.use_pca:\n",
    "            return self.pca.transform(X_scaled)\n",
    "        return X_scaled\n",
    "\n",
    "    def get_reconstruction_error(self, X):\n",
    "        \"\"\"Compute per-observation reconstruction error (MSE).\"\"\"\n",
    "        X_transformed = self.transform(X)\n",
    "        X_reconstructed = self.model.predict(X_transformed)\n",
    "        mse = np.mean((X_transformed - X_reconstructed) ** 2, axis=1)\n",
    "        return mse\n",
    "\n",
    "    def score_anomalies(self, X, contamination=0.05):\n",
    "        \"\"\"Score and flag anomalies based on reconstruction error.\"\"\"\n",
    "        errors = self.get_reconstruction_error(X)\n",
    "\n",
    "        # Normalize to 0-100 scale\n",
    "        scores = 100 * (errors - errors.min()) / (errors.max() - errors.min() + 1e-10)\n",
    "\n",
    "        # Flag top contamination%\n",
    "        threshold = np.percentile(errors, 100 * (1 - contamination))\n",
    "        is_anomaly = errors >= threshold\n",
    "\n",
    "        return scores, is_anomaly, errors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# LOCAL OUTLIER FACTOR\n",
    "# =============================================================================\n",
    "\n",
    "def run_lof(X, n_neighbors=12, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Run Local Outlier Factor for comparison.\n",
    "    Catches context-dependent outliers that global methods miss.\n",
    "    \"\"\"\n",
    "    # Ensure n_neighbors doesn't exceed sample size - 1\n",
    "    effective_neighbors = min(n_neighbors, X.shape[0] - 1)\n",
    "\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=effective_neighbors,\n",
    "        contamination=contamination,\n",
    "        novelty=False\n",
    "    )\n",
    "\n",
    "    labels = lof.fit_predict(X)\n",
    "\n",
    "    # Negative outlier factor (more negative = more anomalous)\n",
    "    scores_raw = -lof.negative_outlier_factor_\n",
    "\n",
    "    # Normalize to 0-100\n",
    "    scores = 100 * (scores_raw - scores_raw.min()) / (scores_raw.max() - scores_raw.min() + 1e-10)\n",
    "\n",
    "    is_anomaly = labels == -1\n",
    "\n",
    "    return scores, is_anomaly\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENSEMBLE SCORING\n",
    "# =============================================================================\n",
    "\n",
    "def compute_ensemble_score(nn_scores, lof_scores, nn_weight=0.6, lof_weight=0.4):\n",
    "    \"\"\"Combine NN and LOF scores with NN weighted higher.\"\"\"\n",
    "    return nn_weight * nn_scores + lof_weight * lof_scores\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PER-BANK REPORT GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_bank_report(bank_name, df, df_flagged, feature_cols,\n",
    "                         autoencoder, training_info, lof_info):\n",
    "    \"\"\"Generate a detailed anomaly report for one bank.\"\"\"\n",
    "    lines = [\n",
    "        \"=\" * 70,\n",
    "        f\"NEURAL NETWORK ANOMALY DETECTION REPORT: {bank_name.upper()}\",\n",
    "        f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"=\" * 70,\n",
    "        \"\",\n",
    "        \"DATA CHARACTERISTICS\",\n",
    "        \"-\" * 40,\n",
    "        f\"Observations (quarters): {len(df)}\",\n",
    "        f\"Original QoQ features: {len(feature_cols)}\",\n",
    "        f\"Obs/feature ratio: {len(df)/max(len(feature_cols),1):.2f}\",\n",
    "        f\"PCA components ({int(PCA_VARIANCE*100)}% var): {autoencoder.n_components_}\",\n",
    "        f\"Obs/PCA-component ratio: {len(df)/max(autoencoder.n_components_,1):.2f}\",\n",
    "        \"\",\n",
    "        \"MODEL ARCHITECTURE (Track 2 - Per-Bank, Small Sample)\",\n",
    "        \"-\" * 40,\n",
    "        f\"Type: Shallow Autoencoder\",\n",
    "        f\"Hidden layers: {HIDDEN_LAYERS}\",\n",
    "        f\"Bottleneck dimension: {HIDDEN_LAYERS[1]}\",\n",
    "        f\"L2 regularization (alpha): {ALPHA}\",\n",
    "        f\"PCA preprocessing: Yes ({autoencoder.n_components_} components)\",\n",
    "        f\"Validation fraction: 20%\",\n",
    "        \"\",\n",
    "        \"TRAINING INFO\",\n",
    "        \"-\" * 40,\n",
    "        f\"Iterations: {training_info['n_iter']}\",\n",
    "        f\"Final loss: {training_info['loss']:.6f}\",\n",
    "        f\"Converged: {training_info['converged']}\",\n",
    "        \"\",\n",
    "        \"ENSEMBLE APPROACH\",\n",
    "        \"-\" * 40,\n",
    "        f\"  1. Shallow Autoencoder ({int(NN_WEIGHT*100)}% weight)\",\n",
    "        f\"  2. Local Outlier Factor ({int(LOF_WEIGHT*100)}% weight)\",\n",
    "        f\"  LOF neighbors: {LOF_NEIGHBORS}\",\n",
    "        f\"  LOF anomalies: {lof_info['n_anomalies']}\",\n",
    "        \"\",\n",
    "        \"RESULTS\",\n",
    "        \"-\" * 40,\n",
    "        f\"Total quarters analyzed: {len(df)}\",\n",
    "        f\"Anomalies flagged (ensemble): {len(df_flagged)} ({len(df_flagged)/max(len(df),1):.1%})\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    # List all flagged anomalies for this bank\n",
    "    if len(df_flagged) > 0:\n",
    "        lines.append(\"FLAGGED ANOMALOUS QUARTERS\")\n",
    "        lines.append(\"-\" * 40)\n",
    "        for _, row in df_flagged.iterrows():\n",
    "            lines.append(\n",
    "                f\"  {row['ensemble_score']:.1f}: {row[QUARTER_COLUMN]}\"\n",
    "                f\"  (NN: {row['nn_score']:.1f}, LOF: {row['lof_score']:.1f})\"\n",
    "            )\n",
    "        lines.append(\"\")\n",
    "\n",
    "    # Check stress period coverage\n",
    "    lines.append(\"VALIDATION: STRESS PERIOD COVERAGE\")\n",
    "    lines.append(\"-\" * 40)\n",
    "    lines.append(\"Known stress periods that should show elevated scores:\")\n",
    "    lines.append(\"  - 2008-2009: Financial Crisis\")\n",
    "    lines.append(\"  - 2020: COVID-19 pandemic\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    stress_periods = ['2008', '2009', '2010', '2020']\n",
    "    for period in stress_periods:\n",
    "        period_mask = df[QUARTER_COLUMN].astype(str).str.contains(period)\n",
    "        period_data = df[period_mask]\n",
    "        if len(period_data) > 0:\n",
    "            n_flagged = period_data['is_anomaly'].sum()\n",
    "            avg_score = period_data['ensemble_score'].mean()\n",
    "            lines.append(f\"  {period}: {n_flagged} flagged, avg score {avg_score:.1f}\")\n",
    "        else:\n",
    "            lines.append(f\"  {period}: no data\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CROSS-BANK SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "def generate_cross_bank_summary(all_results):\n",
    "    \"\"\"Generate a comparison report across all banks.\"\"\"\n",
    "    lines = [\n",
    "        \"=\" * 70,\n",
    "        \"NEURAL NETWORK ANOMALY DETECTION: CROSS-BANK SUMMARY (Track 2)\",\n",
    "        f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"=\" * 70,\n",
    "        \"\",\n",
    "        \"APPROACH\",\n",
    "        \"-\" * 40,\n",
    "        \"Each bank has its own autoencoder trained only on its own history.\",\n",
    "        \"Anomalies reflect quarters unusual for THAT bank, not cross-bank.\",\n",
    "        f\"Architecture: {HIDDEN_LAYERS} | Alpha: {ALPHA} | PCA: {int(PCA_VARIANCE*100)}%\",\n",
    "        \"\",\n",
    "        \"PER-BANK OVERVIEW\",\n",
    "        \"-\" * 70,\n",
    "        f\"{'Bank':<30} {'Obs':>5} {'Feat':>5} {'PCA':>5} {'Anom':>5} {'MaxScore':>9}\",\n",
    "        \"-\" * 70,\n",
    "    ]\n",
    "\n",
    "    for bank_name, res in all_results.items():\n",
    "        df = res['df_all']\n",
    "        n_anom = df['is_anomaly'].sum()\n",
    "        max_score = df['ensemble_score'].max()\n",
    "        lines.append(\n",
    "            f\"{bank_name:<30} {res['n_obs']:>5} {res['n_features']:>5} \"\n",
    "            f\"{res['n_pca']:>5} {n_anom:>5} {max_score:>9.1f}\"\n",
    "        )\n",
    "\n",
    "    lines.extend([\"\", \"\"])\n",
    "\n",
    "    # Top anomalies across all banks\n",
    "    lines.append(\"TOP 10 ANOMALIES ACROSS ALL BANKS\")\n",
    "    lines.append(\"-\" * 70)\n",
    "\n",
    "    all_flagged = []\n",
    "    for bank_name, res in all_results.items():\n",
    "        df = res['df_all'].copy()\n",
    "        df['_bank_name'] = bank_name\n",
    "        all_flagged.append(df)\n",
    "\n",
    "    if all_flagged:\n",
    "        combined = pd.concat(all_flagged, ignore_index=True)\n",
    "        top10 = combined.nlargest(10, 'ensemble_score')\n",
    "        for _, row in top10.iterrows():\n",
    "            lines.append(\n",
    "                f\"  {row['ensemble_score']:.1f}: {row['_bank_name']} - {row[QUARTER_COLUMN]}\"\n",
    "                f\"  (NN: {row['nn_score']:.1f}, LOF: {row['lof_score']:.1f})\"\n",
    "            )\n",
    "\n",
    "    lines.extend([\"\", \"\"])\n",
    "\n",
    "    # Stress period comparison\n",
    "    lines.append(\"STRESS PERIOD COMPARISON\")\n",
    "    lines.append(\"-\" * 70)\n",
    "    lines.append(f\"{'Bank':<30} {'2008':>6} {'2009':>6} {'2010':>6} {'2020':>6}\")\n",
    "    lines.append(\"-\" * 70)\n",
    "\n",
    "    for bank_name, res in all_results.items():\n",
    "        df = res['df_all']\n",
    "        counts = []\n",
    "        for period in ['2008', '2009', '2010', '2020']:\n",
    "            mask = df[QUARTER_COLUMN].astype(str).str.contains(period)\n",
    "            n = df.loc[mask, 'is_anomaly'].sum() if mask.any() else 0\n",
    "            counts.append(str(int(n)))\n",
    "        lines.append(f\"{bank_name:<30} {counts[0]:>6} {counts[1]:>6} {counts[2]:>6} {counts[3]:>6}\")\n",
    "\n",
    "    lines.extend([\"\", \"\"])\n",
    "\n",
    "    # Model convergence comparison\n",
    "    lines.append(\"MODEL CONVERGENCE\")\n",
    "    lines.append(\"-\" * 70)\n",
    "    lines.append(f\"{'Bank':<30} {'Iters':>6} {'Loss':>10} {'Converged':>10}\")\n",
    "    lines.append(\"-\" * 70)\n",
    "\n",
    "    for bank_name, res in all_results.items():\n",
    "        ti = res['training_info']\n",
    "        lines.append(\n",
    "            f\"{bank_name:<30} {ti['n_iter']:>6} {ti['loss']:>10.6f} \"\n",
    "            f\"{'Yes' if ti['converged'] else 'No':>10}\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESS ONE BANK\n",
    "# =============================================================================\n",
    "\n",
    "def process_bank(filepath, bank_name):\n",
    "    \"\"\"\n",
    "    Run the full autoencoder + LOF ensemble pipeline on a single bank's\n",
    "    QoQ CSV and return all results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  {bank_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Load data (handle .xls files that are actually CSVs)\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(filepath, engine='xlrd')\n",
    "    print(f\"  Loaded: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Detect data orientation and normalize to: rows=quarters, cols=features\n",
    "    # New format: rows=features, cols=quarters, with 'feature' column\n",
    "    # Old format: rows=quarters, cols=features, with 'IDRSSD'/'quarter'\n",
    "    # Detection: if 'feature' column exists and contains _qoq values, transpose\n",
    "    # -------------------------------------------------------------------------\n",
    "    if 'feature' in df.columns:\n",
    "        has_qoq_features = df['feature'].astype(str).str.contains('_qoq').any()\n",
    "        if has_qoq_features:\n",
    "            print(\"  Detected transposed format (features as rows). Transposing...\")\n",
    "            feature_names = df['feature'].tolist()\n",
    "            df_t = df.drop(columns=['feature']).T\n",
    "            df_t.columns = feature_names\n",
    "            df_t.index.name = 'quarter'\n",
    "            df_t = df_t.reset_index()\n",
    "            df_t.rename(columns={'index': 'quarter'}, inplace=True)\n",
    "            df = df_t\n",
    "            print(f\"  After transpose: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "\n",
    "    # Get QoQ feature columns\n",
    "    feature_cols = [c for c in df.columns if c.endswith('_qoq')]\n",
    "    n_features = len(feature_cols)\n",
    "    print(f\"  QoQ features: {n_features}\")\n",
    "    print(f\"  Quarters: {len(df)}\")\n",
    "    print(f\"  Obs/feature ratio: {len(df)/max(n_features,1):.2f}\")\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "\n",
    "    # Force numeric (transposed data may have string types)\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Handle missing/infinite values\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    for col in feature_cols:\n",
    "        if X[col].isnull().any():\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "    # Check if we have enough data (quarters)\n",
    "    n_quarters = len(df)\n",
    "    if n_quarters < 10:\n",
    "        print(f\"  WARNING: Only {n_quarters} quarters. Results may be unreliable.\")\n",
    "    if n_features == 0:\n",
    "        print(f\"  ERROR: No QoQ feature columns found. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODEL 1: Shallow Autoencoder with PCA\n",
    "    # =========================================================================\n",
    "    print(f\"\\n  [1/2] Training Shallow Autoencoder...\")\n",
    "    print(f\"    Architecture: {HIDDEN_LAYERS} ({HIDDEN_LAYERS[1]}-dim bottleneck)\")\n",
    "    print(f\"    L2 regularization: {ALPHA}\")\n",
    "    print(f\"    PCA: {int(PCA_VARIANCE*100)}% variance retention\")\n",
    "\n",
    "    autoencoder = ShallowAutoencoder(\n",
    "        hidden_layers=HIDDEN_LAYERS,\n",
    "        alpha=ALPHA,\n",
    "        max_iter=MAX_ITER,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        use_pca=True,\n",
    "        pca_variance=PCA_VARIANCE,\n",
    "        pca_min_components=PCA_MIN_COMPONENTS\n",
    "    )\n",
    "\n",
    "    # Reduce validation fraction for very small samples (e.g., Goldman at 67 quarters)\n",
    "    if n_quarters < 80:\n",
    "        autoencoder.model.set_params(validation_fraction=0.15)\n",
    "        print(f\"    Validation fraction reduced to 15% (small sample)\")\n",
    "\n",
    "    autoencoder.fit(X)\n",
    "\n",
    "    training_info = {\n",
    "        'n_iter': autoencoder.model.n_iter_,\n",
    "        'loss': autoencoder.model.loss_,\n",
    "        'converged': autoencoder.model.n_iter_ < MAX_ITER\n",
    "    }\n",
    "    print(f\"    PCA components: {autoencoder.n_components_}\")\n",
    "    print(f\"    Training iterations: {training_info['n_iter']}\")\n",
    "    print(f\"    Final loss: {training_info['loss']:.6f}\")\n",
    "\n",
    "    nn_scores, nn_anomaly, nn_errors = autoencoder.score_anomalies(X, CONTAMINATION)\n",
    "    print(f\"    NN anomalies: {nn_anomaly.sum()}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODEL 2: Local Outlier Factor\n",
    "    # =========================================================================\n",
    "    print(f\"\\n  [2/2] Running Local Outlier Factor...\")\n",
    "\n",
    "    X_scaled = autoencoder.scaler.transform(X)\n",
    "\n",
    "    # Apply PCA to LOF input as well for consistency\n",
    "    if autoencoder.use_pca:\n",
    "        X_lof = autoencoder.pca.transform(X_scaled)\n",
    "    else:\n",
    "        X_lof = X_scaled\n",
    "\n",
    "    lof_scores, lof_anomaly = run_lof(X_lof, LOF_NEIGHBORS, LOF_CONTAMINATION)\n",
    "    lof_info = {'n_anomalies': int(lof_anomaly.sum())}\n",
    "    print(f\"    LOF anomalies: {lof_info['n_anomalies']}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # ENSEMBLE\n",
    "    # =========================================================================\n",
    "    print(f\"\\n  Combining scores (NN: {int(NN_WEIGHT*100)}%, LOF: {int(LOF_WEIGHT*100)}%)...\")\n",
    "    ensemble_scores = compute_ensemble_score(nn_scores, lof_scores, NN_WEIGHT, LOF_WEIGHT)\n",
    "\n",
    "    threshold = np.percentile(ensemble_scores, 100 * (1 - CONTAMINATION))\n",
    "    ensemble_anomaly = ensemble_scores >= threshold\n",
    "\n",
    "    # Add scores to dataframe\n",
    "    df['nn_score'] = nn_scores\n",
    "    df['lof_score'] = lof_scores\n",
    "    df['ensemble_score'] = ensemble_scores\n",
    "    df['nn_anomaly'] = nn_anomaly\n",
    "    df['lof_anomaly'] = lof_anomaly\n",
    "    df['is_anomaly'] = ensemble_anomaly\n",
    "    df['reconstruction_error'] = nn_errors\n",
    "    df['anomaly_score'] = ensemble_scores  # backwards compatibility\n",
    "\n",
    "    print(f\"  Ensemble anomalies: {ensemble_anomaly.sum()}\")\n",
    "\n",
    "    both_flag = (nn_anomaly & lof_anomaly).sum()\n",
    "    print(f\"  Flagged by BOTH models: {both_flag} (high confidence)\")\n",
    "\n",
    "    df_flagged = df[df['is_anomaly']].sort_values('ensemble_score', ascending=False)\n",
    "\n",
    "    # Generate per-bank report\n",
    "    report = generate_bank_report(\n",
    "        bank_name, df, df_flagged, feature_cols,\n",
    "        autoencoder, training_info, lof_info\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'df_all': df,\n",
    "        'df_flagged': df_flagged,\n",
    "        'feature_cols': feature_cols,\n",
    "        'n_obs': len(df),\n",
    "        'n_features': n_features,\n",
    "        'n_pca': autoencoder.n_components_,\n",
    "        'training_info': training_info,\n",
    "        'lof_info': lof_info,\n",
    "        'report': report,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FFIEC NEURAL NETWORK ANOMALY DETECTION (Track 2 - Per-Bank)\")\n",
    "    print(\"Shallow Autoencoder + LOF Ensemble, Trained Individually\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Discover per-bank QoQ files (.csv or .xls that are actually CSVs)\n",
    "    bank_files = sorted(\n",
    "        list(INPUT_DIR.glob(\"ffiec_*_qoq.csv\")) +\n",
    "        list(INPUT_DIR.glob(\"ffiec_*_qoq.xls\"))\n",
    "    )\n",
    "\n",
    "    if not bank_files:\n",
    "        print(f\"\\nERROR: No per-bank QoQ files found in {INPUT_DIR}/\")\n",
    "        print(\"  Expected files like: ffiec_bank_of_america_qoq.csv (or .xls)\")\n",
    "        print(\"  Run Step 4 Track 2 first.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nFound {len(bank_files)} per-bank QoQ files in {INPUT_DIR}/:\")\n",
    "    for f in bank_files:\n",
    "        print(f\"  {f.name}\")\n",
    "\n",
    "    # Process each bank\n",
    "    all_results = {}\n",
    "\n",
    "    for filepath in bank_files:\n",
    "        # Derive bank slug and display name from filename\n",
    "        # e.g., ffiec_bank_of_america_qoq.csv -> bank_of_america\n",
    "        #   or  ffiec_bank_of_america_qoq.xls -> bank_of_america\n",
    "        slug = filepath.stem.replace('ffiec_', '').replace('_qoq', '')\n",
    "        bank_name = slug.replace('_', ' ').title()\n",
    "\n",
    "        # Try to match to canonical name from BANKS dict\n",
    "        for rssd, name in BANKS.items():\n",
    "            candidate_slug = name.lower().replace(' ', '_').replace('.', '')\n",
    "            if candidate_slug == slug:\n",
    "                bank_name = name\n",
    "                break\n",
    "\n",
    "        result = process_bank(filepath, bank_name)\n",
    "\n",
    "        if result is None:\n",
    "            print(f\"  SKIPPED: {bank_name} (insufficient data)\")\n",
    "            continue\n",
    "\n",
    "        all_results[bank_name] = result\n",
    "\n",
    "        # Save per-bank outputs\n",
    "        out_all = OUTPUT_DIR / f\"ffiec_{slug}_nn_anomalies.csv\"\n",
    "        out_flagged = OUTPUT_DIR / f\"ffiec_{slug}_nn_anomalies_flagged.csv\"\n",
    "        out_report = OUTPUT_DIR / f\"nn_report_{slug}.txt\"\n",
    "\n",
    "        result['df_all'].to_csv(out_all, index=False)\n",
    "        result['df_flagged'].to_csv(out_flagged, index=False)\n",
    "        with open(out_report, 'w') as f:\n",
    "            f.write(result['report'])\n",
    "\n",
    "        print(f\"\\n  Saved: {out_all.name} ({len(result['df_all'])} rows)\")\n",
    "        print(f\"  Saved: {out_flagged.name} ({len(result['df_flagged'])} rows)\")\n",
    "        print(f\"  Saved: {out_report.name}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # CROSS-BANK SUMMARY\n",
    "    # =========================================================================\n",
    "    if all_results:\n",
    "        print(\"\\n\\n\" + \"=\" * 70)\n",
    "        print(\"CROSS-BANK SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        summary = generate_cross_bank_summary(all_results)\n",
    "        summary_path = OUTPUT_DIR / \"nn_cross_bank_summary.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Saved: {summary_path.name}\")\n",
    "\n",
    "        # Print summary table\n",
    "        print(f\"\\n{'Bank':<30} {'Obs':>5} {'Feat':>5} {'PCA':>5} {'Anom':>5} {'MaxScore':>9}\")\n",
    "        print(\"-\" * 70)\n",
    "        for bank_name, res in all_results.items():\n",
    "            df = res['df_all']\n",
    "            n_anom = df['is_anomaly'].sum()\n",
    "            max_score = df['ensemble_score'].max()\n",
    "            print(\n",
    "                f\"{bank_name:<30} {res['n_obs']:>5} {res['n_features']:>5} \"\n",
    "                f\"{res['n_pca']:>5} {n_anom:>5} {max_score:>9.1f}\"\n",
    "            )\n",
    "\n",
    "        # Top anomalies\n",
    "        print(f\"\\nTOP 10 ANOMALIES ACROSS ALL BANKS:\")\n",
    "        all_dfs = []\n",
    "        for bank_name, res in all_results.items():\n",
    "            tmp = res['df_all'].copy()\n",
    "            tmp['_bank_name'] = bank_name\n",
    "            all_dfs.append(tmp)\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        for _, row in combined.nlargest(10, 'ensemble_score').iterrows():\n",
    "            print(f\"  {row['ensemble_score']:.1f}: {row['_bank_name']} - {row[QUARTER_COLUMN]}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPLETE\")\n",
    "    print(f\"All outputs saved to {OUTPUT_DIR.resolve()}/\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64178af-4cee-404d-af28-3dca2f18d7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
