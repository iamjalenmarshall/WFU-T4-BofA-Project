{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe75ebb1-6fc8-42a1-84eb-e51fb35b4558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ffiec-data-collector in /home/haleop21/.local/lib/python3.11/site-packages (2.0.0rc2)\n",
      "Requirement already satisfied: requests>=2.28.0 in /deac/opt/rocky9-noarch/python/3.11.8/lib/python3.11/site-packages (from ffiec-data-collector) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /deac/opt/rocky9-noarch/python/3.11.8/lib/python3.11/site-packages (from ffiec-data-collector) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /deac/opt/rocky9-noarch/python/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.8.0->ffiec-data-collector) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /deac/opt/rocky9-noarch/python/3.11.8/lib/python3.11/site-packages (from requests>=2.28.0->ffiec-data-collector) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /deac/opt/rocky9-noarch/python/3.11.8/lib/python3.11/site-packages (from requests>=2.28.0->ffiec-data-collector) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /deac/opt/rocky9-noarch/python/3.11.8/lib/python3.11/site-packages (from requests>=2.28.0->ffiec-data-collector) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /deac/opt/rocky9-noarch/python/3.11.8/lib/python3.11/site-packages (from requests>=2.28.0->ffiec-data-collector) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ffiec-data-collector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b8f99-9691-4cf9-84b4-ade63f50f235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2705895/865336935.py:26: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "2026-01-20 16:25:47,019 [INFO] Discovering available FFIEC quarters...\n",
      "2026-01-20 16:25:47,212 [INFO] FFIEC offers 99 total quarters.\n",
      "2026-01-20 16:25:47,212 [INFO] Will download 99 quarters\n",
      "2026-01-20 16:25:47,212 [INFO] Date range: 03/31/2001 to 12/31/2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FFIEC COMPLETE DATA DOWNLOADER (Step 1)\n",
      "ALL BANKS - NO FILTERING\n",
      "======================================================================\n",
      "\n",
      "CONFIGURATION:\n",
      "----------------------------------------\n",
      "Quarters to download: 100\n",
      "Output: ffiec_all_banks.csv\n",
      "NOTE: This will download ALL banks (~5,000 per quarter)\n",
      "\n",
      "\n",
      "Estimated time: 16.5 hours at 6 req/hr\n",
      "----------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:   0%|          | 0/99 [00:00<?, ?quarter/s]2026-01-20 16:25:47,256 [INFO] Requesting quarter 12/31/2024...\n",
      "2026-01-20 16:26:03,334 [INFO]   -> 4543 banks, 4004 columns\n",
      "Downloading:   1%|          | 1/99 [00:16<26:15, 16.08s/quarter]2026-01-20 16:26:03,336 [INFO] Requesting quarter 12/31/2023...\n",
      "2026-01-20 16:36:04,912 [INFO]   -> 4642 banks, 4095 columns\n",
      "Downloading:   2%|▏         | 2/99 [10:17<9:42:47, 360.49s/quarter]2026-01-20 16:36:04,914 [INFO] Requesting quarter 12/31/2022...\n",
      "2026-01-20 16:46:06,032 [INFO]   -> 4756 banks, 4165 columns\n",
      "Downloading:   3%|▎         | 3/99 [20:18<12:32:35, 470.37s/quarter]2026-01-20 16:46:06,034 [INFO] Requesting quarter 12/31/2021...\n",
      "2026-01-20 16:56:07,218 [INFO]   -> 4887 banks, 4166 columns\n",
      "Downloading:   4%|▍         | 4/99 [30:19<13:46:31, 522.01s/quarter]2026-01-20 16:56:07,220 [INFO] Requesting quarter 12/31/2020...\n",
      "2026-01-20 17:06:08,427 [INFO]   -> 5050 banks, 4151 columns\n",
      "Downloading:   5%|▌         | 5/99 [40:21<14:22:33, 550.57s/quarter]2026-01-20 17:06:08,428 [INFO] Requesting quarter 12/31/2019...\n",
      "2026-01-20 17:16:09,756 [INFO]   -> 5227 banks, 4124 columns\n",
      "Downloading:   6%|▌         | 6/99 [50:22<14:40:08, 567.83s/quarter]2026-01-20 17:16:09,757 [INFO] Requesting quarter 12/31/2018...\n",
      "2026-01-20 17:26:09,646 [INFO]   -> 5456 banks, 4034 columns\n",
      "Downloading:   7%|▋         | 7/99 [1:00:22<14:46:44, 578.31s/quarter]2026-01-20 17:26:09,648 [INFO] Requesting quarter 12/31/2017...\n",
      "2026-01-20 17:36:12,394 [INFO]   -> 5721 banks, 4394 columns\n",
      "Downloading:   8%|▊         | 8/99 [1:10:25<14:48:54, 586.09s/quarter]2026-01-20 17:36:12,396 [INFO] Requesting quarter 12/31/2016...\n",
      "2026-01-20 17:46:13,136 [INFO]   -> 5966 banks, 4338 columns\n",
      "Downloading:   9%|▉         | 9/99 [1:20:25<14:46:00, 590.67s/quarter]2026-01-20 17:46:13,138 [INFO] Requesting quarter 12/31/2015...\n",
      "2026-01-20 17:56:14,676 [INFO]   -> 6238 banks, 4382 columns\n",
      "Downloading:  10%|█         | 10/99 [1:30:27<14:41:08, 594.03s/quarter]2026-01-20 17:56:14,677 [INFO] Requesting quarter 12/31/2014...\n",
      "2026-01-20 18:06:15,467 [INFO]   -> 6570 banks, 4080 columns\n",
      "Downloading:  11%|█         | 11/99 [1:40:28<14:34:16, 596.10s/quarter]2026-01-20 18:06:15,468 [INFO] Requesting quarter 12/31/2013...\n",
      "2026-01-20 18:16:15,516 [INFO]   -> 6877 banks, 3932 columns\n",
      "Downloading:  12%|█▏        | 12/99 [1:50:28<14:26:04, 597.30s/quarter]2026-01-20 18:16:15,517 [INFO] Requesting quarter 12/31/2012...\n",
      "2026-01-20 18:26:13,969 [INFO]   -> 7150 banks, 3527 columns\n",
      "Downloading:  13%|█▎        | 13/99 [2:00:26<14:16:37, 597.65s/quarter]2026-01-20 18:26:13,970 [INFO] Requesting quarter 12/31/2011...\n",
      "2026-01-20 18:36:13,362 [INFO]   -> 6789 banks, 3500 columns\n",
      "Downloading:  14%|█▍        | 14/99 [2:10:26<14:07:24, 598.18s/quarter]2026-01-20 18:36:13,363 [INFO] Requesting quarter 12/31/2010...\n",
      "2026-01-20 18:46:12,029 [INFO]   -> 6999 banks, 3057 columns\n",
      "Downloading:  15%|█▌        | 15/99 [2:20:24<13:57:39, 598.32s/quarter]2026-01-20 18:46:12,031 [INFO] Requesting quarter 12/31/2009...\n",
      "2026-01-20 18:56:13,332 [INFO]   -> 7321 banks, 3026 columns\n",
      "Downloading:  16%|█▌        | 16/99 [2:30:26<13:48:55, 599.22s/quarter]2026-01-20 18:56:13,333 [INFO] Requesting quarter 12/31/2008...\n",
      "2026-01-20 19:06:10,622 [INFO]   -> 7568 banks, 2394 columns\n",
      "Downloading:  17%|█▋        | 17/99 [2:40:23<13:38:08, 598.64s/quarter]2026-01-20 19:06:10,623 [INFO] Requesting quarter 12/31/2007...\n",
      "2026-01-20 19:16:10,425 [INFO]   -> 7788 banks, 2223 columns\n",
      "Downloading:  18%|█▊        | 18/99 [2:50:23<13:28:38, 598.99s/quarter]2026-01-20 19:16:10,426 [INFO] Requesting quarter 12/31/2006...\n",
      "2026-01-20 19:26:10,853 [INFO]   -> 7922 banks, 2101 columns\n",
      "Downloading:  19%|█▉        | 19/99 [3:00:23<13:19:13, 599.42s/quarter]2026-01-20 19:26:10,854 [INFO] Requesting quarter 12/31/2005...\n",
      "2026-01-20 19:36:14,818 [INFO]   -> 8056 banks, 2076 columns\n",
      "Downloading:  20%|██        | 20/99 [3:10:27<13:11:02, 600.79s/quarter]2026-01-20 19:36:14,820 [INFO] Requesting quarter 12/31/2004...\n",
      "2026-01-20 19:46:15,725 [INFO]   -> 8179 banks, 2053 columns\n",
      "Downloading:  21%|██        | 21/99 [3:20:28<13:01:04, 600.82s/quarter]2026-01-20 19:46:15,727 [INFO] Requesting quarter 12/31/2003...\n",
      "2026-01-20 19:56:16,865 [INFO]   -> 8348 banks, 2047 columns\n",
      "Downloading:  22%|██▏       | 22/99 [3:30:29<12:51:10, 600.92s/quarter]2026-01-20 19:56:16,867 [INFO] Requesting quarter 12/31/2002...\n",
      "2026-01-20 20:06:18,574 [INFO]   -> 8468 banks, 2011 columns\n",
      "Downloading:  23%|██▎       | 23/99 [3:40:31<12:41:27, 601.15s/quarter]2026-01-20 20:06:18,575 [INFO] Requesting quarter 12/31/2001...\n",
      "2026-01-20 20:16:19,690 [INFO]   -> 8689 banks, 1969 columns\n",
      "Downloading:  24%|██▍       | 24/99 [3:50:32<12:31:25, 601.14s/quarter]2026-01-20 20:16:19,691 [INFO] Requesting quarter 09/30/2025...\n",
      "2026-01-20 20:26:23,717 [INFO]   -> 4435 banks, 4004 columns\n",
      "Downloading:  25%|██▌       | 25/99 [4:00:36<12:22:28, 602.01s/quarter]2026-01-20 20:26:23,718 [INFO] Requesting quarter 09/30/2024...\n",
      "2026-01-20 20:36:24,682 [INFO]   -> 4572 banks, 3966 columns\n",
      "Downloading:  26%|██▋       | 26/99 [4:10:37<12:12:03, 601.70s/quarter]2026-01-20 20:36:24,683 [INFO] Requesting quarter 09/30/2023...\n",
      "2026-01-20 20:46:27,278 [INFO]   -> 4670 banks, 4095 columns\n",
      "Downloading:  27%|██▋       | 27/99 [4:20:40<12:02:21, 601.97s/quarter]2026-01-20 20:46:27,279 [INFO] Requesting quarter 09/30/2022...\n",
      "2026-01-20 20:56:28,766 [INFO]   -> 4796 banks, 4168 columns\n",
      "Downloading:  28%|██▊       | 28/99 [4:30:41<11:52:09, 601.82s/quarter]2026-01-20 20:56:28,768 [INFO] Requesting quarter 09/30/2021...\n",
      "2026-01-20 21:06:30,160 [INFO]   -> 4962 banks, 4165 columns\n",
      "Downloading:  29%|██▉       | 29/99 [4:40:42<11:41:58, 601.69s/quarter]2026-01-20 21:06:30,161 [INFO] Requesting quarter 09/30/2020...\n",
      "2026-01-20 21:16:31,242 [INFO]   -> 5082 banks, 4160 columns\n",
      "Downloading:  30%|███       | 30/99 [4:50:43<11:31:44, 601.51s/quarter]2026-01-20 21:16:31,243 [INFO] Requesting quarter 09/30/2019...\n",
      "2026-01-20 21:26:32,643 [INFO]   -> 5308 banks, 4124 columns\n",
      "Downloading:  31%|███▏      | 31/99 [5:00:45<11:21:40, 601.48s/quarter]2026-01-20 21:26:32,644 [INFO] Requesting quarter 09/30/2018...\n",
      "2026-01-20 21:36:32,926 [INFO]   -> 5527 banks, 4036 columns\n",
      "Downloading:  32%|███▏      | 32/99 [5:10:45<11:11:14, 601.12s/quarter]2026-01-20 21:36:32,927 [INFO] Requesting quarter 09/30/2017...\n",
      "2026-01-20 21:46:36,018 [INFO]   -> 5789 banks, 4394 columns\n",
      "Downloading:  33%|███▎      | 33/99 [5:20:48<11:01:52, 601.71s/quarter]2026-01-20 21:46:36,019 [INFO] Requesting quarter 09/30/2016...\n",
      "2026-01-20 21:56:36,586 [INFO]   -> 6034 banks, 4338 columns\n",
      "Downloading:  34%|███▍      | 34/99 [5:30:49<10:51:28, 601.37s/quarter]2026-01-20 21:56:36,587 [INFO] Requesting quarter 09/30/2015...\n",
      "2026-01-20 22:06:38,508 [INFO]   -> 6328 banks, 4382 columns\n",
      "Downloading:  35%|███▌      | 35/99 [5:40:51<10:41:38, 601.53s/quarter]2026-01-20 22:06:38,509 [INFO] Requesting quarter 09/30/2014...\n",
      "2026-01-20 22:16:39,605 [INFO]   -> 6651 banks, 4080 columns\n",
      "Downloading:  36%|███▋      | 36/99 [5:50:52<10:31:28, 601.40s/quarter]2026-01-20 22:16:39,606 [INFO] Requesting quarter 09/30/2013...\n",
      "2026-01-20 22:26:39,281 [INFO]   -> 6955 banks, 3932 columns\n",
      "Downloading:  37%|███▋      | 37/99 [6:00:52<10:20:54, 600.88s/quarter]2026-01-20 22:26:39,282 [INFO] Requesting quarter 09/30/2012...\n",
      "2026-01-20 22:36:37,883 [INFO]   -> 7249 banks, 3527 columns\n",
      "Downloading:  38%|███▊      | 38/99 [6:10:50<10:10:12, 600.20s/quarter]2026-01-20 22:36:37,884 [INFO] Requesting quarter 09/30/2011...\n",
      "2026-01-20 22:46:36,995 [INFO]   -> 6820 banks, 3500 columns\n",
      "Downloading:  39%|███▉      | 39/99 [6:20:49<9:59:52, 599.87s/quarter] 2026-01-20 22:46:36,996 [INFO] Requesting quarter 09/30/2010...\n",
      "2026-01-20 22:56:35,291 [INFO]   -> 7094 banks, 3055 columns\n",
      "Downloading:  40%|████      | 40/99 [6:30:48<9:49:24, 599.40s/quarter]2026-01-20 22:56:35,293 [INFO] Requesting quarter 09/30/2009...\n",
      "2026-01-20 23:06:35,310 [INFO]   -> 7393 banks, 2917 columns\n",
      "Downloading:  41%|████▏     | 41/99 [6:40:48<9:39:35, 599.59s/quarter]2026-01-20 23:06:35,312 [INFO] Requesting quarter 09/30/2008...\n",
      "2026-01-20 23:16:32,987 [INFO]   -> 7640 banks, 2392 columns\n",
      "Downloading:  42%|████▏     | 42/99 [6:50:45<9:29:03, 599.01s/quarter]2026-01-20 23:16:32,988 [INFO] Requesting quarter 09/30/2007...\n",
      "2026-01-20 23:26:32,316 [INFO]   -> 7811 banks, 2223 columns\n",
      "Downloading:  43%|████▎     | 43/99 [7:00:45<9:19:10, 599.11s/quarter]2026-01-20 23:26:32,317 [INFO] Requesting quarter 09/30/2006...\n",
      "2026-01-20 23:36:32,723 [INFO]   -> 7978 banks, 2139 columns\n",
      "Downloading:  44%|████▍     | 44/99 [7:10:45<9:09:32, 599.50s/quarter]2026-01-20 23:36:32,724 [INFO] Requesting quarter 09/30/2005...\n",
      "2026-01-20 23:46:32,813 [INFO]   -> 8079 banks, 2076 columns\n",
      "Downloading:  45%|████▌     | 45/99 [7:20:45<8:59:42, 599.68s/quarter]2026-01-20 23:46:32,814 [INFO] Requesting quarter 09/30/2004...\n",
      "2026-01-20 23:56:33,305 [INFO]   -> 8219 banks, 2053 columns\n",
      "Downloading:  46%|████▋     | 46/99 [7:30:46<8:49:55, 599.92s/quarter]2026-01-20 23:56:33,306 [INFO] Requesting quarter 09/30/2003...\n",
      "2026-01-21 00:06:33,930 [INFO]   -> 8391 banks, 2047 columns\n",
      "Downloading:  47%|████▋     | 47/99 [7:40:46<8:40:06, 600.13s/quarter]2026-01-21 00:06:33,932 [INFO] Requesting quarter 09/30/2002...\n",
      "2026-01-21 00:16:34,380 [INFO]   -> 8522 banks, 2015 columns\n",
      "Downloading:  48%|████▊     | 48/99 [7:50:47<8:30:11, 600.23s/quarter]2026-01-21 00:16:34,381 [INFO] Requesting quarter 09/30/2001...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FFIEC Complete Data Downloader (Step 1)\n",
    "=======================================\n",
    "Purpose: Download ALL available FFIEC Call Report data for ALL banks\n",
    "         and save to a single CSV file.\n",
    "\n",
    "This script downloads CALL_SINGLE for each quarter, extracts ALL banks' data,\n",
    "and combines everything into a single wide-format CSV.\n",
    "\n",
    "Output: ffiec_all_banks.csv\n",
    "        (One row per bank per quarter, thousands of columns from all schedules)\n",
    "\n",
    "NOTE: This file will be LARGE (~500MB - 1GB) but only needs to be run ONCE.\n",
    "      Use Step 2 to filter to specific banks for analysis.\n",
    "\n",
    "Author: Wake Forest MSBA Practicum Team 4\n",
    "Date: January 2026\n",
    "\"\"\"\n",
    "\n",
    "import zipfile\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ffiec_data_collector import (\n",
    "    FFIECDownloader,\n",
    "    Product,\n",
    "    FileFormat,\n",
    "    WebpageChangeException,\n",
    ")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Number of quarters to download\n",
    "N_QUARTERS = 100  # ~25 years\n",
    "\n",
    "# Output paths\n",
    "DOWNLOAD_DIR = Path(\"./ffiec_downloads\")\n",
    "OUTPUT_CSV = Path(\"./ffiec_all_banks.csv\")\n",
    "\n",
    "# Rate limiting (FFIEC guideline: ~6 requests per hour for large files)\n",
    "REQUESTS_PER_HOUR = 6\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF_FACTOR = 2.0\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "# Create directories\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RATE-LIMITED DOWNLOADER\n",
    "# =============================================================================\n",
    "\n",
    "class RateLimitedDownloader:\n",
    "    \"\"\"Wrapper around FFIECDownloader that respects rate limits.\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_hour=REQUESTS_PER_HOUR, download_dir=DOWNLOAD_DIR):\n",
    "        self.downloader = FFIECDownloader(download_dir=download_dir)\n",
    "        self.last_request_time = None\n",
    "        self.min_interval = 3600.0 / max(1, requests_per_hour)\n",
    "\n",
    "    def _sleep_if_needed(self):\n",
    "        if self.last_request_time is None:\n",
    "            return\n",
    "        elapsed = (pd.Timestamp.now() - self.last_request_time).total_seconds()\n",
    "        if elapsed < self.min_interval:\n",
    "            sleep_time = self.min_interval - elapsed\n",
    "            logging.debug(f\"Sleeping {sleep_time:.1f}s to respect rate limits\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    def download_with_retries(self, product, period, file_format, max_retries=MAX_RETRIES):\n",
    "        attempt = 0\n",
    "        while attempt <= max_retries:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                self._sleep_if_needed()\n",
    "                logging.debug(f\"Attempt {attempt} downloading {period}\")\n",
    "\n",
    "                result = self.downloader.download(\n",
    "                    product=product,\n",
    "                    period=period,\n",
    "                    format=file_format,\n",
    "                )\n",
    "\n",
    "                self.last_request_time = pd.Timestamp.now()\n",
    "\n",
    "                if result.success:\n",
    "                    return result\n",
    "\n",
    "                if result.error_message and (\n",
    "                    \"404\" in result.error_message or \n",
    "                    \"not found\" in result.error_message.lower()\n",
    "                ):\n",
    "                    logging.warning(f\"{period}: File not available.\")\n",
    "                    return None\n",
    "\n",
    "                logging.warning(f\"{period}: Error: {result.error_message}\")\n",
    "\n",
    "            except WebpageChangeException:\n",
    "                logging.error(\"FFIEC website structure changed; aborting.\")\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"{period}: Exception: {e}\")\n",
    "\n",
    "            if attempt <= max_retries:\n",
    "                backoff = RETRY_BACKOFF_FACTOR ** (attempt - 1)\n",
    "                logging.info(f\"Retrying in {backoff:.0f}s...\")\n",
    "                time.sleep(backoff)\n",
    "\n",
    "        logging.error(f\"All retries failed for {period}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DATA EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "def extract_all_schedules_from_zip(zip_path):\n",
    "    \"\"\"\n",
    "    Extract data for ALL banks from a CALL_SINGLE ZIP file.\n",
    "    \n",
    "    The ZIP contains multiple schedule .txt files. We read each one\n",
    "    and merge them all together on IDRSSD.\n",
    "    \n",
    "    Returns: DataFrame with one row per bank, all schedule columns merged\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schedule_dfs = []\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            txt_files = [f for f in zf.namelist() if f.lower().endswith(\".txt\")]\n",
    "            \n",
    "            if not txt_files:\n",
    "                logging.warning(f\"{zip_path.name}: No .txt files found\")\n",
    "                return None\n",
    "            \n",
    "            for txt_file in txt_files:\n",
    "                try:\n",
    "                    with zf.open(txt_file) as fh:\n",
    "                        content = fh.read().decode('utf-8', errors='replace')\n",
    "                    \n",
    "                    # Read with first row as header, skip second row (descriptions)\n",
    "                    df = pd.read_csv(\n",
    "                        StringIO(content),\n",
    "                        sep='\\t',\n",
    "                        header=0,\n",
    "                        skiprows=[1],  # Skip the description row\n",
    "                        dtype=str,\n",
    "                        low_memory=False\n",
    "                    )\n",
    "                    \n",
    "                    df.columns = df.columns.str.strip()\n",
    "                    \n",
    "                    # Find RSSD column\n",
    "                    rssd_col = next((c for c in df.columns if 'rssd' in c.lower()), None)\n",
    "                    if rssd_col is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Standardize to IDRSSD\n",
    "                    if rssd_col != 'IDRSSD':\n",
    "                        df = df.rename(columns={rssd_col: 'IDRSSD'})\n",
    "                    \n",
    "                    df['IDRSSD'] = df['IDRSSD'].astype(str).str.strip()\n",
    "                    \n",
    "                    # NO FILTERING - keep all banks!\n",
    "                    \n",
    "                    # Handle duplicate column names within this file\n",
    "                    cols = df.columns.tolist()\n",
    "                    seen = {}\n",
    "                    new_cols = []\n",
    "                    for col in cols:\n",
    "                        if col in seen:\n",
    "                            seen[col] += 1\n",
    "                            new_cols.append(f\"{col}_{seen[col]}\")\n",
    "                        else:\n",
    "                            seen[col] = 0\n",
    "                            new_cols.append(col)\n",
    "                    df.columns = new_cols\n",
    "                    \n",
    "                    schedule_dfs.append(df)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.debug(f\"Error reading {txt_file}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if not schedule_dfs:\n",
    "            return None\n",
    "        \n",
    "        # Merge all schedules on IDRSSD\n",
    "        merged = schedule_dfs[0]\n",
    "        for i, df in enumerate(schedule_dfs[1:], 2):\n",
    "            # Handle overlapping columns\n",
    "            overlap = set(merged.columns) & set(df.columns) - {'IDRSSD'}\n",
    "            if overlap:\n",
    "                df = df.rename(columns={col: f\"{col}_dup{i}\" for col in overlap})\n",
    "            merged = merged.merge(df, on='IDRSSD', how='outer')\n",
    "        \n",
    "        # Keep only valid RSSD rows (numeric IDs)\n",
    "        merged = merged[merged['IDRSSD'].str.match(r'^\\d+$', na=False)]\n",
    "        \n",
    "        return merged\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Failed processing {zip_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FFIEC COMPLETE DATA DOWNLOADER (Step 1)\")\n",
    "    print(\"ALL BANKS - NO FILTERING\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Display configuration\n",
    "    print(\"CONFIGURATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Quarters to download: {N_QUARTERS}\")\n",
    "    print(f\"Output: {OUTPUT_CSV}\")\n",
    "    print(f\"NOTE: This will download ALL banks (~5,000 per quarter)\")\n",
    "    print()\n",
    "    \n",
    "    # Discover available quarters\n",
    "    logging.info(\"Discovering available FFIEC quarters...\")\n",
    "    tmp = FFIECDownloader(download_dir=DOWNLOAD_DIR)\n",
    "    periods = tmp.select_product(Product.CALL_SINGLE)\n",
    "    \n",
    "    available_quarters = sorted([p.date_str for p in periods], reverse=True)\n",
    "    logging.info(f\"FFIEC offers {len(available_quarters)} total quarters.\")\n",
    "    \n",
    "    quarters_to_download = available_quarters[:N_QUARTERS]\n",
    "    \n",
    "    if not quarters_to_download:\n",
    "        logging.error(\"No quarters available!\")\n",
    "        return None\n",
    "    \n",
    "    logging.info(f\"Will download {len(quarters_to_download)} quarters\")\n",
    "    logging.info(f\"Date range: {quarters_to_download[-1]} to {quarters_to_download[0]}\")\n",
    "    \n",
    "    # Estimate time\n",
    "    est_hours = len(quarters_to_download) * (3600 / REQUESTS_PER_HOUR) / 3600\n",
    "    print(f\"\\nEstimated time: {est_hours:.1f} hours at {REQUESTS_PER_HOUR} req/hr\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "    \n",
    "    # Initialize downloader\n",
    "    rl = RateLimitedDownloader(\n",
    "        requests_per_hour=REQUESTS_PER_HOUR,\n",
    "        download_dir=DOWNLOAD_DIR\n",
    "    )\n",
    "    \n",
    "    # Download and process each quarter\n",
    "    all_quarters = []\n",
    "    \n",
    "    for quarter in tqdm(quarters_to_download, desc=\"Downloading\", unit=\"quarter\"):\n",
    "        logging.info(f\"Requesting quarter {quarter}...\")\n",
    "        \n",
    "        result = rl.download_with_retries(\n",
    "            product=Product.CALL_SINGLE,\n",
    "            period=quarter,\n",
    "            file_format=FileFormat.TSV,\n",
    "        )\n",
    "        \n",
    "        if result is None or not getattr(result, \"success\", False):\n",
    "            logging.info(f\"Skipping {quarter} (download failed)\")\n",
    "            continue\n",
    "        \n",
    "        # Extract all schedules for ALL banks\n",
    "        df_quarter = extract_all_schedules_from_zip(Path(result.file_path))\n",
    "        \n",
    "        if df_quarter is None or df_quarter.empty:\n",
    "            logging.info(f\"No data in {quarter}\")\n",
    "            continue\n",
    "        \n",
    "        # Add quarter identifier\n",
    "        df_quarter['quarter'] = quarter\n",
    "        all_quarters.append(df_quarter)\n",
    "        \n",
    "        logging.info(f\"  -> {len(df_quarter)} banks, {len(df_quarter.columns)} columns\")\n",
    "    \n",
    "    # Combine all quarters\n",
    "    if all_quarters:\n",
    "        print(\"\\nCombining all quarters...\")\n",
    "        final_df = pd.concat(all_quarters, ignore_index=True)\n",
    "        \n",
    "        # Reorder columns: IDRSSD, quarter, then everything else alphabetically\n",
    "        id_cols = ['IDRSSD', 'quarter']\n",
    "        other_cols = sorted([c for c in final_df.columns if c not in id_cols])\n",
    "        final_df = final_df[id_cols + other_cols]\n",
    "        \n",
    "        # Save\n",
    "        print(f\"Saving to {OUTPUT_CSV}...\")\n",
    "        final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Output: {OUTPUT_CSV}\")\n",
    "        print(f\"Shape: {final_df.shape[0]:,} rows x {final_df.shape[1]:,} columns\")\n",
    "        print(f\"Quarters: {final_df['quarter'].nunique()}\")\n",
    "        print(f\"Unique banks: {final_df['IDRSSD'].nunique():,}\")\n",
    "        \n",
    "        # Show quarters covered\n",
    "        quarters_covered = sorted(final_df['quarter'].unique())\n",
    "        print(f\"\\nQuarters: {quarters_covered[0]} to {quarters_covered[-1]}\")\n",
    "        \n",
    "        # File size\n",
    "        file_size_mb = OUTPUT_CSV.stat().st_size / (1024**2)\n",
    "        print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        logging.error(\"No data collected!\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a090f1-f193-4aac-8bab-fa95c7280116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
