{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6481feb1-3608-44e1-8038-83229b5f6054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FFIEC FEATURE FILTERING PIPELINE (Step 3)\n",
      "======================================================================\n",
      "\n",
      "Loading ffiec_complete_call_reports.csv...\n",
      "  Loaded: 99 rows x 6,444 columns\n",
      "\n",
      "Categorizing columns...\n",
      "  Metadata columns:    11\n",
      "  Numeric columns:     5,731\n",
      "  Non-numeric columns: 702\n",
      "\n",
      "Filtering by null percentage (threshold: 0%)...\n",
      "  Kept: 462  |  Removed: 5,269\n",
      "\n",
      "Filtering by variance (threshold: 0.001)...\n",
      "  Kept: 433  |  Removed: 29\n",
      "\n",
      "Filtering by unique value ratio (threshold: 0.01)...\n",
      "  Kept: 430  |  Removed: 3\n",
      "\n",
      "Filtering duplicate columns...\n",
      "  Kept: 430  |  Removed: 0\n",
      "\n",
      "Building output dataset...\n",
      "\n",
      "Saving filtered data to ffiec_filtered_features.csv...\n",
      "  Shape: 99 rows x 441 columns\n",
      "Saving filtering report to filtering_report.txt...\n",
      "\n",
      "======================================================================\n",
      "COMPLETE\n",
      "======================================================================\n",
      "Original:  6,444 columns\n",
      "Final:     441 columns (430 features + 11 metadata)\n",
      "Reduction: 93.2%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "FEATURE COLUMNS FOR ANOMALY DETECTION:\n",
      "----------------------------------------\n",
      "    1. RCFD0010\n",
      "    2. RCFD0022\n",
      "    3. RCFD0071\n",
      "    4. RCFD0081\n",
      "    5. RCFD0090\n",
      "    6. RCFD0211\n",
      "    7. RCFD0213\n",
      "    8. RCFD0416\n",
      "    9. RCFD1248\n",
      "   10. RCFD1249\n",
      "   11. RCFD1250\n",
      "   12. RCFD1251\n",
      "   13. RCFD1252\n",
      "   14. RCFD1253\n",
      "   15. RCFD1254\n",
      "   16. RCFD1255\n",
      "   17. RCFD1256\n",
      "   18. RCFD1286\n",
      "   19. RCFD1287\n",
      "   20. RCFD1583\n",
      "   21. RCFD1590\n",
      "   22. RCFD1594\n",
      "   23. RCFD1597\n",
      "   24. RCFD1737\n",
      "   25. RCFD1738\n",
      "   26. RCFD1739\n",
      "   27. RCFD1741\n",
      "   28. RCFD1742\n",
      "   29. RCFD1743\n",
      "   30. RCFD1744\n",
      "   31. RCFD1746\n",
      "   32. RCFD1752\n",
      "   33. RCFD1754\n",
      "   34. RCFD1763\n",
      "   35. RCFD1764\n",
      "   36. RCFD1771\n",
      "   37. RCFD1772\n",
      "   38. RCFD1773\n",
      "   39. RCFD2081\n",
      "   40. RCFD2107\n",
      "   41. RCFD2122\n",
      "   42. RCFD2130\n",
      "   43. RCFD2145\n",
      "   44. RCFD2148\n",
      "   45. RCFD2150\n",
      "   46. RCFD2160\n",
      "   47. RCFD2168\n",
      "   48. RCFD2170\n",
      "   49. RCFD2651\n",
      "   50. RCFD2746\n",
      "  ... and 380 more\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FFIEC Call Report Feature Filtering Script (Step 3 - Updated)\n",
    "==============================================================\n",
    "Purpose: Take the raw combined FFIEC CSV and filter down to\n",
    "         a clean set of numeric features suitable for anomaly detection.\n",
    "\n",
    "Approach: Pure data-driven filtering - no subjective domain judgment.\n",
    "          Let the data tell us which columns are useful.\n",
    "\n",
    "Input:  ffiec_data/ffiec_complete_call_reports.csv (from Step 1)\n",
    "        - 99 rows (quarters) x 6,444 columns\n",
    "        \n",
    "Output: ffiec_filtered_features.csv (same rows, fewer columns)\n",
    "        filtering_report.txt (documents what was removed and why)\n",
    "\n",
    "Author: Wake Forest MSBA Practicum Team 4\n",
    "Date: January 2026\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these thresholds as needed\n",
    "# =============================================================================\n",
    "\n",
    "# Path to your combined CSV file (from Step 1)\n",
    "INPUT_FILE = \"ffiec_complete_call_reports.csv\"\n",
    "\n",
    "# Output files\n",
    "OUTPUT_FILE = \"ffiec_filtered_features.csv\"\n",
    "REPORT_FILE = \"filtering_report.txt\"\n",
    "\n",
    "# Filtering thresholds\n",
    "NULL_THRESHOLD = 0.0        # Drop columns with ANY null values (complete case analysis)\n",
    "VARIANCE_THRESHOLD = 0.001  # Drop columns with near-zero variance (after scaling)\n",
    "UNIQUE_RATIO_THRESHOLD = 0.01  # Adjusted for 99 rows (need at least ~1 unique value per 100 rows)\n",
    "\n",
    "# Columns to always exclude (metadata, identifiers - not features)\n",
    "# These are kept in the output but not used for anomaly detection\n",
    "METADATA_COLUMNS = [\n",
    "    'IDRSSD',                           # Bank identifier - need this for grouping\n",
    "    'quarter',                          # Time period - need this for QoQ analysis\n",
    "    'FDIC Certificate Number',\n",
    "    'OCC Charter Number', \n",
    "    'OTS Docket Number',\n",
    "    'Primary ABA Routing Number',\n",
    "    'Financial Institution Name',\n",
    "    'Financial Institution Address',\n",
    "    'Financial Institution City',\n",
    "    'Financial Institution State',\n",
    "    'Financial Institution Zip Code',\n",
    "]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load the combined FFIEC CSV file.\n",
    "    \n",
    "    Note: low_memory=False is important because FFIEC data has mixed types\n",
    "    in some columns (numbers stored as strings, etc.)\n",
    "    \"\"\"\n",
    "    print(f\"Loading {filepath}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    print(f\"  Loaded: {df.shape[0]:,} rows x {df.shape[1]:,} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def identify_column_types(df):\n",
    "    \"\"\"\n",
    "    Categorize columns into metadata, numeric, and non-numeric.\n",
    "    \n",
    "    FFIEC columns follow patterns:\n",
    "    - RCFD/RCON = Balance sheet items (domestic/foreign consolidated)\n",
    "    - RIAD/RIAE = Income statement items\n",
    "    - UBPR = Uniform Bank Performance Report ratios\n",
    "    \n",
    "    Returns dict with column lists.\n",
    "    \"\"\"\n",
    "    all_cols = set(df.columns)\n",
    "    \n",
    "    # Metadata columns (identifiers, not features)\n",
    "    metadata = set(METADATA_COLUMNS) & all_cols\n",
    "    \n",
    "    # Find numeric columns (potential features)\n",
    "    # Try to convert each column to numeric - if it works, it's numeric\n",
    "    numeric_cols = []\n",
    "    non_numeric_cols = []\n",
    "    \n",
    "    for col in all_cols - metadata:\n",
    "        # Skip columns that are clearly junk (unnamed columns from parsing errors)\n",
    "        if col.startswith('Unnamed:'):\n",
    "            non_numeric_cols.append(col)\n",
    "            continue\n",
    "            \n",
    "        # Try to infer if column is numeric\n",
    "        # Check if pandas thinks it's numeric OR if it can be converted\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            numeric_cols.append(col)\n",
    "        else:\n",
    "            # Try converting - some numeric data is stored as strings\n",
    "            try:\n",
    "                converted = pd.to_numeric(df[col], errors='coerce')\n",
    "                # If more than 50% converted successfully, treat as numeric\n",
    "                if converted.notna().mean() > 0.5:\n",
    "                    numeric_cols.append(col)\n",
    "                else:\n",
    "                    non_numeric_cols.append(col)\n",
    "            except:\n",
    "                non_numeric_cols.append(col)\n",
    "    \n",
    "    return {\n",
    "        'metadata': list(metadata),\n",
    "        'numeric': numeric_cols,\n",
    "        'non_numeric': non_numeric_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_by_null_percentage(df, columns, threshold):\n",
    "    \"\"\"\n",
    "    Remove columns with too many null values.\n",
    "    \n",
    "    Why: Columns with >50% nulls are often:\n",
    "    - Line items only reported by certain bank types\n",
    "    - Discontinued reporting items\n",
    "    - Items only reported in certain quarters\n",
    "    \n",
    "    These create problems for anomaly detection (imputation would dominate)\n",
    "    \n",
    "    Returns: (kept_columns, removed_columns_with_reasons)\n",
    "    \"\"\"\n",
    "    null_pct = df[columns].isnull().mean()\n",
    "    \n",
    "    kept = null_pct[null_pct <= threshold].index.tolist()\n",
    "    removed = null_pct[null_pct > threshold]\n",
    "    \n",
    "    removed_with_reasons = {\n",
    "        col: f\"Null percentage: {pct:.1%}\" \n",
    "        for col, pct in removed.items()\n",
    "    }\n",
    "    \n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def filter_by_variance(df, columns, threshold):\n",
    "    \"\"\"\n",
    "    Remove columns with near-zero variance.\n",
    "    \n",
    "    Why: Columns where almost every value is the same provide no signal\n",
    "    for anomaly detection. Common causes:\n",
    "    - Flag columns that are almost always 0 or 1\n",
    "    - Line items that apply to very few banks\n",
    "    - Deprecated items filled with a constant\n",
    "    \n",
    "    Process:\n",
    "    1. Standardize the column (subtract mean, divide by std)\n",
    "    2. Compute variance of standardized values\n",
    "    3. If variance is near zero, the column is effectively constant\n",
    "    \n",
    "    Returns: (kept_columns, removed_columns_with_reasons)\n",
    "    \"\"\"\n",
    "    kept = []\n",
    "    removed_with_reasons = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        series = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Skip if all null after conversion\n",
    "        if series.notna().sum() == 0:\n",
    "            removed_with_reasons[col] = \"All values null after numeric conversion\"\n",
    "            continue\n",
    "        \n",
    "        # Compute variance (handle edge cases)\n",
    "        std = series.std()\n",
    "        \n",
    "        if std == 0 or pd.isna(std):\n",
    "            # Constant column - zero variance\n",
    "            removed_with_reasons[col] = f\"Zero variance (constant value: {series.dropna().iloc[0] if series.notna().any() else 'N/A'})\"\n",
    "        else:\n",
    "            # Normalize and check variance\n",
    "            normalized = (series - series.mean()) / std\n",
    "            var = normalized.var()\n",
    "            \n",
    "            if var < threshold:\n",
    "                removed_with_reasons[col] = f\"Near-zero variance: {var:.6f}\"\n",
    "            else:\n",
    "                kept.append(col)\n",
    "    \n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def filter_by_unique_ratio(df, columns, threshold):\n",
    "    \"\"\"\n",
    "    Remove columns with very few unique values relative to dataset size.\n",
    "    \n",
    "    Why: Columns with only a handful of distinct values (e.g., 3 unique values\n",
    "    across 99 rows) behave more like categorical flags than continuous\n",
    "    features. They can dominate anomaly detection in misleading ways.\n",
    "    \n",
    "    This catches things like:\n",
    "    - Boolean flags (only 0/1)\n",
    "    - Code columns (only a few valid codes)\n",
    "    - Binned/categorical data stored as numbers\n",
    "    \n",
    "    Returns: (kept_columns, removed_columns_with_reasons)\n",
    "    \"\"\"\n",
    "    kept = []\n",
    "    removed_with_reasons = {}\n",
    "    \n",
    "    n_rows = len(df)\n",
    "    \n",
    "    for col in columns:\n",
    "        series = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        \n",
    "        if len(series) == 0:\n",
    "            removed_with_reasons[col] = \"No valid numeric values\"\n",
    "            continue\n",
    "            \n",
    "        n_unique = series.nunique()\n",
    "        ratio = n_unique / n_rows\n",
    "        \n",
    "        # Also check absolute number - if only 2-3 unique values, suspicious\n",
    "        if n_unique <= 3:\n",
    "            removed_with_reasons[col] = f\"Only {n_unique} unique values (likely categorical/flag)\"\n",
    "        elif ratio < threshold:\n",
    "            removed_with_reasons[col] = f\"Low unique ratio: {n_unique} unique / {n_rows} rows = {ratio:.6f}\"\n",
    "        else:\n",
    "            kept.append(col)\n",
    "    \n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def filter_duplicate_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Remove columns that are duplicates of each other.\n",
    "    \n",
    "    Why: The FFIEC data merge process created some duplicate columns \n",
    "    (e.g., RCON1234_dup2, RCON1234_dup3). These are redundant and \n",
    "    will artificially inflate the importance of those items in \n",
    "    anomaly detection.\n",
    "    \n",
    "    Strategy: Keep the first occurrence, remove _dup versions if \n",
    "    they're highly correlated with the original.\n",
    "    \n",
    "    Returns: (kept_columns, removed_columns_with_reasons)\n",
    "    \"\"\"\n",
    "    kept = []\n",
    "    removed_with_reasons = {}\n",
    "    \n",
    "    # First pass: identify base names and their duplicates\n",
    "    base_names = {}  # base_name -> [col1, col2_dup, col3_dup, ...]\n",
    "    \n",
    "    for col in columns:\n",
    "        # Check if this is a duplicate column\n",
    "        dup_match = re.match(r'(.+)_dup\\d+$', col)\n",
    "        if dup_match:\n",
    "            base = dup_match.group(1)\n",
    "            if base not in base_names:\n",
    "                base_names[base] = []\n",
    "            base_names[base].append(col)\n",
    "        else:\n",
    "            # This might be a base column\n",
    "            if col not in base_names:\n",
    "                base_names[col] = []\n",
    "    \n",
    "    # Second pass: for each group, keep only one representative\n",
    "    seen_bases = set()\n",
    "    \n",
    "    for col in columns:\n",
    "        dup_match = re.match(r'(.+)_dup\\d+$', col)\n",
    "        \n",
    "        if dup_match:\n",
    "            base = dup_match.group(1)\n",
    "            if base in seen_bases:\n",
    "                # We already have this base column, skip the duplicate\n",
    "                removed_with_reasons[col] = f\"Duplicate of {base}\"\n",
    "            else:\n",
    "                # Base doesn't exist in our columns, keep this dup as representative\n",
    "                kept.append(col)\n",
    "                seen_bases.add(base)\n",
    "        else:\n",
    "            # Not a duplicate column\n",
    "            kept.append(col)\n",
    "            seen_bases.add(col)\n",
    "    \n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def convert_to_numeric(df, columns):\n",
    "    \"\"\"\n",
    "    Convert selected columns to numeric dtype.\n",
    "    \n",
    "    Some FFIEC columns are stored as strings but contain numeric data.\n",
    "    This ensures consistent numeric types for modeling.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df_out.columns:\n",
    "            df_out[col] = pd.to_numeric(df_out[col], errors='coerce')\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def generate_report(original_count, col_types, filter_results, final_columns):\n",
    "    \"\"\"\n",
    "    Generate a human-readable report of what was filtered and why.\n",
    "    \"\"\"\n",
    "    lines = [\n",
    "        \"=\" * 70,\n",
    "        \"FFIEC FEATURE FILTERING REPORT\",\n",
    "        f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"=\" * 70,\n",
    "        \"\",\n",
    "        \"SUMMARY\",\n",
    "        \"-\" * 40,\n",
    "        f\"Original columns:           {original_count:,}\",\n",
    "        f\"Metadata columns (kept):    {len(col_types['metadata']):,}\",\n",
    "        f\"Non-numeric (removed):      {len(col_types['non_numeric']):,}\",\n",
    "        f\"Numeric columns analyzed:   {len(col_types['numeric']):,}\",\n",
    "        f\"Final feature columns:      {len(final_columns):,}\",\n",
    "        \"\",\n",
    "        \"FILTERING STEPS\",\n",
    "        \"-\" * 40,\n",
    "    ]\n",
    "    \n",
    "    # Document each filtering step\n",
    "    for step_name, (kept_count, removed_dict) in filter_results.items():\n",
    "        lines.append(f\"\\n{step_name}:\")\n",
    "        lines.append(f\"  Kept: {kept_count:,} columns\")\n",
    "        lines.append(f\"  Removed: {len(removed_dict):,} columns\")\n",
    "        \n",
    "        # Show sample of removed columns (not all - could be thousands)\n",
    "        if removed_dict:\n",
    "            lines.append(\"  Sample of removed columns:\")\n",
    "            for i, (col, reason) in enumerate(list(removed_dict.items())[:10]):\n",
    "                lines.append(f\"    - {col}: {reason}\")\n",
    "            if len(removed_dict) > 10:\n",
    "                lines.append(f\"    ... and {len(removed_dict) - 10} more\")\n",
    "    \n",
    "    # List final columns\n",
    "    lines.extend([\n",
    "        \"\",\n",
    "        \"FINAL FEATURE COLUMNS\",\n",
    "        \"-\" * 40,\n",
    "        f\"Total: {len(final_columns)} columns\",\n",
    "        \"\",\n",
    "    ])\n",
    "    \n",
    "    # Group columns by prefix for readability\n",
    "    prefixes = {}\n",
    "    for col in sorted(final_columns):\n",
    "        prefix = col[:4] if len(col) >= 4 else col\n",
    "        if prefix not in prefixes:\n",
    "            prefixes[prefix] = []\n",
    "        prefixes[prefix].append(col)\n",
    "    \n",
    "    for prefix in sorted(prefixes.keys()):\n",
    "        cols = prefixes[prefix]\n",
    "        lines.append(f\"{prefix}*: {len(cols)} columns\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main filtering pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load raw data\n",
    "    2. Categorize columns (metadata, numeric, non-numeric)\n",
    "    3. Filter numeric columns by:\n",
    "       a. Null percentage (remove >50% null)\n",
    "       b. Variance (remove near-constant columns)\n",
    "       c. Unique ratio (remove quasi-categorical columns)\n",
    "       d. Duplicates (remove _dup columns)\n",
    "    4. Save filtered dataset and report\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FFIEC FEATURE FILTERING PIPELINE (Step 3)\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    df = load_data(INPUT_FILE)\n",
    "    original_count = len(df.columns)\n",
    "    \n",
    "    # Step 2: Categorize columns\n",
    "    print(\"\\nCategorizing columns...\")\n",
    "    col_types = identify_column_types(df)\n",
    "    print(f\"  Metadata columns:    {len(col_types['metadata']):,}\")\n",
    "    print(f\"  Numeric columns:     {len(col_types['numeric']):,}\")\n",
    "    print(f\"  Non-numeric columns: {len(col_types['non_numeric']):,}\")\n",
    "    \n",
    "    # Track filtering results for report\n",
    "    filter_results = {}\n",
    "    current_columns = col_types['numeric']\n",
    "    \n",
    "    # Step 3a: Filter by null percentage\n",
    "    print(f\"\\nFiltering by null percentage (threshold: {NULL_THRESHOLD:.0%})...\")\n",
    "    kept, removed = filter_by_null_percentage(df, current_columns, NULL_THRESHOLD)\n",
    "    filter_results['Null Percentage Filter'] = (len(kept), removed)\n",
    "    print(f\"  Kept: {len(kept):,}  |  Removed: {len(removed):,}\")\n",
    "    current_columns = kept\n",
    "    \n",
    "    # Step 3b: Filter by variance\n",
    "    print(f\"\\nFiltering by variance (threshold: {VARIANCE_THRESHOLD})...\")\n",
    "    kept, removed = filter_by_variance(df, current_columns, VARIANCE_THRESHOLD)\n",
    "    filter_results['Variance Filter'] = (len(kept), removed)\n",
    "    print(f\"  Kept: {len(kept):,}  |  Removed: {len(removed):,}\")\n",
    "    current_columns = kept\n",
    "    \n",
    "    # Step 3c: Filter by unique ratio\n",
    "    print(f\"\\nFiltering by unique value ratio (threshold: {UNIQUE_RATIO_THRESHOLD})...\")\n",
    "    kept, removed = filter_by_unique_ratio(df, current_columns, UNIQUE_RATIO_THRESHOLD)\n",
    "    filter_results['Unique Ratio Filter'] = (len(kept), removed)\n",
    "    print(f\"  Kept: {len(kept):,}  |  Removed: {len(removed):,}\")\n",
    "    current_columns = kept\n",
    "    \n",
    "    # Step 3d: Filter duplicates\n",
    "    print(\"\\nFiltering duplicate columns...\")\n",
    "    kept, removed = filter_duplicate_columns(df, current_columns)\n",
    "    filter_results['Duplicate Filter'] = (len(kept), removed)\n",
    "    print(f\"  Kept: {len(kept):,}  |  Removed: {len(removed):,}\")\n",
    "    final_feature_columns = kept\n",
    "    \n",
    "    # Step 4: Build output dataframe\n",
    "    print(\"\\nBuilding output dataset...\")\n",
    "    \n",
    "    # Output columns = metadata + filtered features\n",
    "    output_columns = col_types['metadata'] + final_feature_columns\n",
    "    output_columns = [c for c in output_columns if c in df.columns]  # Safety check\n",
    "    \n",
    "    df_out = df[output_columns].copy()\n",
    "    \n",
    "    # Convert feature columns to numeric\n",
    "    df_out = convert_to_numeric(df_out, final_feature_columns)\n",
    "    \n",
    "    # Step 5: Save outputs\n",
    "    print(f\"\\nSaving filtered data to {OUTPUT_FILE}...\")\n",
    "    df_out.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"  Shape: {df_out.shape[0]:,} rows x {df_out.shape[1]:,} columns\")\n",
    "    \n",
    "    # Generate and save report\n",
    "    print(f\"Saving filtering report to {REPORT_FILE}...\")\n",
    "    report = generate_report(original_count, col_types, filter_results, final_feature_columns)\n",
    "    with open(REPORT_FILE, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Original:  {original_count:,} columns\")\n",
    "    print(f\"Final:     {len(output_columns):,} columns ({len(final_feature_columns):,} features + {len(col_types['metadata']):,} metadata)\")\n",
    "    print(f\"Reduction: {(1 - len(output_columns)/original_count):.1%}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    return df_out, final_feature_columns\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_filtered, feature_cols = main()\n",
    "    \n",
    "    # Print the feature columns for reference\n",
    "    print(\"\\nFEATURE COLUMNS FOR ANOMALY DETECTION:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, col in enumerate(sorted(feature_cols)[:50], 1):\n",
    "        print(f\"  {i:3}. {col}\")\n",
    "    if len(feature_cols) > 50:\n",
    "        print(f\"  ... and {len(feature_cols) - 50} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb9ddb-be51-454c-b877-31acdb8fd319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
