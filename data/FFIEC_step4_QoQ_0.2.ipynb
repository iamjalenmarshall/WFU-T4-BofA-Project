{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a28922-baa4-41aa-b15f-4cdb7a8b74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FFIEC QUARTER-OVER-QUARTER CHANGE COMPUTATION (v0.2 - FIXED)\n",
      "======================================================================\n",
      "\n",
      "Loading ffiec_filtered_features.csv...\n",
      "  Loaded: 563 rows x 168 columns\n",
      "  Banks: 6\n",
      "  Quarters: 99\n",
      "  Feature columns: 157\n",
      "\n",
      "Converting feature columns to numeric...\n",
      "\n",
      "Parsing quarter dates for sorting...\n",
      "  Quarter order (chronological):\n",
      "    03/31/2001: 03/31/2001\n",
      "    03/31/2002: 03/31/2002\n",
      "    03/31/2003: 03/31/2003\n",
      "    03/31/2004: 03/31/2004\n",
      "    03/31/2005: 03/31/2005\n",
      "    03/31/2006: 03/31/2006\n",
      "    03/31/2007: 03/31/2007\n",
      "    03/31/2008: 03/31/2008\n",
      "    03/31/2009: 03/31/2009\n",
      "    03/31/2010: 03/31/2010\n",
      "    03/31/2011: 03/31/2011\n",
      "    03/31/2012: 03/31/2012\n",
      "    03/31/2013: 03/31/2013\n",
      "    03/31/2014: 03/31/2014\n",
      "    03/31/2015: 03/31/2015\n",
      "    03/31/2016: 03/31/2016\n",
      "    03/31/2017: 03/31/2017\n",
      "    03/31/2018: 03/31/2018\n",
      "    03/31/2019: 03/31/2019\n",
      "    03/31/2020: 03/31/2020\n",
      "    03/31/2021: 03/31/2021\n",
      "    03/31/2022: 03/31/2022\n",
      "    03/31/2023: 03/31/2023\n",
      "    03/31/2024: 03/31/2024\n",
      "    03/31/2025: 03/31/2025\n",
      "    06/30/2001: 06/30/2001\n",
      "    06/30/2002: 06/30/2002\n",
      "    06/30/2003: 06/30/2003\n",
      "    06/30/2004: 06/30/2004\n",
      "    06/30/2005: 06/30/2005\n",
      "    06/30/2006: 06/30/2006\n",
      "    06/30/2007: 06/30/2007\n",
      "    06/30/2008: 06/30/2008\n",
      "    06/30/2009: 06/30/2009\n",
      "    06/30/2010: 06/30/2010\n",
      "    06/30/2011: 06/30/2011\n",
      "    06/30/2012: 06/30/2012\n",
      "    06/30/2013: 06/30/2013\n",
      "    06/30/2014: 06/30/2014\n",
      "    06/30/2015: 06/30/2015\n",
      "    06/30/2016: 06/30/2016\n",
      "    06/30/2017: 06/30/2017\n",
      "    06/30/2018: 06/30/2018\n",
      "    06/30/2019: 06/30/2019\n",
      "    06/30/2020: 06/30/2020\n",
      "    06/30/2021: 06/30/2021\n",
      "    06/30/2022: 06/30/2022\n",
      "    06/30/2023: 06/30/2023\n",
      "    06/30/2024: 06/30/2024\n",
      "    06/30/2025: 06/30/2025\n",
      "    09/30/2001: 09/30/2001\n",
      "    09/30/2002: 09/30/2002\n",
      "    09/30/2003: 09/30/2003\n",
      "    09/30/2004: 09/30/2004\n",
      "    09/30/2005: 09/30/2005\n",
      "    09/30/2006: 09/30/2006\n",
      "    09/30/2007: 09/30/2007\n",
      "    09/30/2008: 09/30/2008\n",
      "    09/30/2009: 09/30/2009\n",
      "    09/30/2010: 09/30/2010\n",
      "    09/30/2011: 09/30/2011\n",
      "    09/30/2012: 09/30/2012\n",
      "    09/30/2013: 09/30/2013\n",
      "    09/30/2014: 09/30/2014\n",
      "    09/30/2015: 09/30/2015\n",
      "    09/30/2016: 09/30/2016\n",
      "    09/30/2017: 09/30/2017\n",
      "    09/30/2018: 09/30/2018\n",
      "    09/30/2019: 09/30/2019\n",
      "    09/30/2020: 09/30/2020\n",
      "    09/30/2021: 09/30/2021\n",
      "    09/30/2022: 09/30/2022\n",
      "    09/30/2023: 09/30/2023\n",
      "    09/30/2024: 09/30/2024\n",
      "    09/30/2025: 09/30/2025\n",
      "    12/31/2001: 12/31/2001\n",
      "    12/31/2002: 12/31/2002\n",
      "    12/31/2003: 12/31/2003\n",
      "    12/31/2004: 12/31/2004\n",
      "    12/31/2005: 12/31/2005\n",
      "    12/31/2006: 12/31/2006\n",
      "    12/31/2007: 12/31/2007\n",
      "    12/31/2008: 12/31/2008\n",
      "    12/31/2009: 12/31/2009\n",
      "    12/31/2010: 12/31/2010\n",
      "    12/31/2011: 12/31/2011\n",
      "    12/31/2012: 12/31/2012\n",
      "    12/31/2013: 12/31/2013\n",
      "    12/31/2014: 12/31/2014\n",
      "    12/31/2015: 12/31/2015\n",
      "    12/31/2016: 12/31/2016\n",
      "    12/31/2017: 12/31/2017\n",
      "    12/31/2018: 12/31/2018\n",
      "    12/31/2019: 12/31/2019\n",
      "    12/31/2020: 12/31/2020\n",
      "    12/31/2021: 12/31/2021\n",
      "    12/31/2022: 12/31/2022\n",
      "    12/31/2023: 12/31/2023\n",
      "    12/31/2024: 12/31/2024\n",
      "\n",
      "Removing banks with fewer than 2 quarters...\n",
      "  Banks removed: 0\n",
      "  Banks remaining: 6\n",
      "\n",
      "Filtering features to those available in all quarters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_5860\\1010331308.py:287: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['quarter_date'] = df[QUARTER_COLUMN].apply(parse_quarter_to_date)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Features with data in all 99 quarters: 157\n",
      "  Features removed (incomplete coverage): 0\n",
      "\n",
      "Sorting data by bank and quarter...\n",
      "\n",
      "Computing quarter-over-quarter changes (with divide-by-zero fix)...\n",
      "  Edge case handling:\n",
      "    - Both zero (0%): 19,555 cells\n",
      "    - From zero (+100.0%): 1,944 cells\n",
      "    - From zero (-100.0%): 149 cells\n",
      "\n",
      "Removing rows with no valid changes (first quarter per bank)...\n",
      "  Rows removed: 6\n",
      "  Rows remaining: 557\n",
      "\n",
      "Winsorizing extreme values at 99th percentile...\n",
      "\n",
      "Saving QoQ changes to ffiec_qoq_changes_v2.csv...\n",
      "  Shape: 557 rows x 159 columns\n",
      "  Remaining NaN values: 0 / 87,449 (0.00%)\n",
      "Saving report to qoq_computation_report.txt...\n",
      "\n",
      "======================================================================\n",
      "COMPLETE\n",
      "======================================================================\n",
      "Input:  ffiec_filtered_features.csv\n",
      "Output: ffiec_qoq_changes_v2.csv\n",
      "  Rows: 557 bank-quarter observations\n",
      "  Columns: 157 QoQ change features\n",
      "======================================================================\n",
      "\n",
      "SAMPLE QoQ CHANGE STATISTICS:\n",
      "----------------------------------------\n",
      "  RCON2236_qoq: mean=+40.43%, std=196.16%, NaN=0\n",
      "  RCONA564_qoq: mean=+45.09%, std=254.96%, NaN=0\n",
      "  RIAD5411_qoq: mean=+28.34%, std=141.12%, NaN=0\n",
      "  RIAD4079_qoq: mean=+15.01%, std=86.25%, NaN=0\n",
      "  RIAD4185_qoq: mean=+54.54%, std=184.94%, NaN=0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FFIEC Quarter-over-Quarter Change Computation (Vectorized) - FIXED VERSION\n",
    "============================================================================\n",
    "Purpose: Transform filtered FFIEC data from raw values to quarter-over-quarter\n",
    "         percentage changes. Anomalies in CHANGES are more meaningful for\n",
    "         audit purposes than anomalies in raw levels.\n",
    "\n",
    "Input:  ffiec_filtered_features.csv (output from filtering script)\n",
    "Output: ffiec_qoq_changes.csv (same banks/quarters, but values are % changes)\n",
    "        qoq_computation_report.txt (documents the transformation)\n",
    "\n",
    "Why QoQ changes?\n",
    "- A bank with $1B in deposits is not \"anomalous\" just because it's large\n",
    "- A bank whose deposits DROPPED 40% in one quarter IS anomalous\n",
    "- Changes normalize for bank size and capture the behavior we care about\n",
    "\n",
    "FIX in v0.2:\n",
    "- Properly handles divide-by-zero cases when previous quarter value is 0\n",
    "- Case 1: previous=0, current=0  -> 0% change (no change)\n",
    "- Case 2: previous=0, current≠0  -> capped at configurable max (default 100%)\n",
    "- Added tracking of how many values were affected by these edge cases\n",
    "\n",
    "Author: Wake Forest MSBA Practicum Team 4\n",
    "Date: January 2026 (Updated February 2026)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "INPUT_FILE = \"ffiec_filtered_features.csv\"\n",
    "OUTPUT_FILE = \"ffiec_qoq_changes_v2.csv\"\n",
    "REPORT_FILE = \"qoq_computation_report.txt\"\n",
    "\n",
    "# Columns that identify the observation (not features)\n",
    "ID_COLUMN = \"IDRSSD\"\n",
    "QUARTER_COLUMN = \"quarter\"\n",
    "\n",
    "# Additional metadata columns to exclude from QoQ computation\n",
    "# These are identifiers that got through filtering because they're numeric\n",
    "EXCLUDE_COLUMNS = [\n",
    "    'FDIC Certificate Number',\n",
    "    'OCC Charter Number',\n",
    "    'OTS Docket Number',\n",
    "    'Primary ABA Routing Number',\n",
    "    'Financial Institution Name',\n",
    "    'Financial Institution Address',\n",
    "    'Financial Institution City',\n",
    "    'Financial Institution State',\n",
    "    'Financial Institution Zip Code',\n",
    "]\n",
    "\n",
    "# How to handle edge cases\n",
    "MIN_QUARTERS_PER_BANK = 2  # Banks with fewer quarters cannot have QoQ changes\n",
    "WINSORIZE_PERCENTILE = 99  # Cap extreme changes at this percentile (both tails)\n",
    "\n",
    "# NEW: Configuration for divide-by-zero handling\n",
    "# When previous=0 and current≠0, what % change should we assign?\n",
    "# Options:\n",
    "#   - A fixed cap (e.g., 100% means \"grew from zero to something\")\n",
    "#   - np.nan to exclude these cases\n",
    "#   - A very large number to flag them as extreme\n",
    "MAX_CHANGE_FROM_ZERO = 100.0  # Cap at 100% when going from 0 to non-zero\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def parse_quarter_to_date(quarter_str):\n",
    "    \"\"\"\n",
    "    Convert FFIEC quarter string to a sortable date.\n",
    "    \n",
    "    Input format: \"FFIEC CDR Call Bulk All Schedules 03312024\"\n",
    "    Output: datetime(2024, 3, 31)\n",
    "    \n",
    "    This ensures quarters sort chronologically, not alphabetically.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d{8})$', quarter_str)\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        # Format: MMDDYYYY\n",
    "        month = int(date_str[0:2])\n",
    "        day = int(date_str[2:4])\n",
    "        year = int(date_str[4:8])\n",
    "        return datetime(year, month, day)\n",
    "    else:\n",
    "        return quarter_str\n",
    "\n",
    "\n",
    "def compute_qoq_changes_safe(current, previous, max_change_from_zero=MAX_CHANGE_FROM_ZERO):\n",
    "    \"\"\"\n",
    "    Compute quarter-over-quarter percentage changes with proper handling of edge cases.\n",
    "    \n",
    "    Formula: (current - previous) / |previous| * 100\n",
    "    \n",
    "    Edge cases handled:\n",
    "    1. previous=0, current=0  -> 0% (no change)\n",
    "    2. previous=0, current>0  -> +max_change_from_zero% (capped positive growth)\n",
    "    3. previous=0, current<0  -> -max_change_from_zero% (capped negative growth)\n",
    "    4. previous=NaN           -> NaN (first quarter, no previous data)\n",
    "    \n",
    "    Returns:\n",
    "        pct_change: DataFrame with percentage changes\n",
    "        stats: dict with counts of how each edge case was handled\n",
    "    \"\"\"\n",
    "    # Initialize stats tracking\n",
    "    stats = {\n",
    "        'total_cells': current.size,\n",
    "        'normal_computation': 0,\n",
    "        'both_zero': 0,\n",
    "        'from_zero_positive': 0,\n",
    "        'from_zero_negative': 0,\n",
    "        'previous_nan': 0,\n",
    "    }\n",
    "    \n",
    "    # Create masks for different cases BEFORE computation\n",
    "    # These need to be computed on the original DataFrames\n",
    "    mask_prev_nan = previous.isna()\n",
    "    mask_prev_zero = (previous == 0) & ~mask_prev_nan\n",
    "    mask_curr_zero = (current == 0)\n",
    "    mask_curr_positive = (current > 0)\n",
    "    mask_curr_negative = (current < 0)\n",
    "    \n",
    "    # Case 1: Both zero -> 0% change\n",
    "    mask_both_zero = mask_prev_zero & mask_curr_zero\n",
    "    \n",
    "    # Case 2: Previous zero, current positive -> cap at +max%\n",
    "    mask_from_zero_positive = mask_prev_zero & mask_curr_positive\n",
    "    \n",
    "    # Case 3: Previous zero, current negative -> cap at -max%\n",
    "    mask_from_zero_negative = mask_prev_zero & mask_curr_negative\n",
    "    \n",
    "    # Compute the standard percentage change\n",
    "    # Suppress warnings for divide-by-zero (we handle it explicitly)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        pct_change = (current - previous) / previous.abs() * 100\n",
    "    \n",
    "    # Replace inf/-inf with NaN temporarily\n",
    "    pct_change = pct_change.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Now apply our edge case fixes\n",
    "    # Case 1: Both zero -> 0%\n",
    "    pct_change = pct_change.where(~mask_both_zero, 0.0)\n",
    "    \n",
    "    # Case 2: From zero to positive -> +max_change_from_zero%\n",
    "    pct_change = pct_change.where(~mask_from_zero_positive, max_change_from_zero)\n",
    "    \n",
    "    # Case 3: From zero to negative -> -max_change_from_zero%\n",
    "    pct_change = pct_change.where(~mask_from_zero_negative, -max_change_from_zero)\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats['both_zero'] = mask_both_zero.sum().sum()\n",
    "    stats['from_zero_positive'] = mask_from_zero_positive.sum().sum()\n",
    "    stats['from_zero_negative'] = mask_from_zero_negative.sum().sum()\n",
    "    stats['previous_nan'] = mask_prev_nan.sum().sum()\n",
    "    stats['normal_computation'] = (\n",
    "        stats['total_cells'] \n",
    "        - stats['both_zero'] \n",
    "        - stats['from_zero_positive'] \n",
    "        - stats['from_zero_negative']\n",
    "        - stats['previous_nan']\n",
    "    )\n",
    "    \n",
    "    return pct_change, stats\n",
    "\n",
    "\n",
    "def generate_report(df_in, df_out, feature_cols, change_cols, banks_removed, edge_case_stats):\n",
    "    \"\"\"\n",
    "    Generate a report documenting the QoQ transformation.\n",
    "    \"\"\"\n",
    "    lines = [\n",
    "        \"=\" * 70,\n",
    "        \"FFIEC QUARTER-OVER-QUARTER CHANGE COMPUTATION REPORT\",\n",
    "        f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"=\" * 70,\n",
    "        \"\",\n",
    "        \"INPUT SUMMARY\",\n",
    "        \"-\" * 40,\n",
    "        f\"Input file: {INPUT_FILE}\",\n",
    "        f\"Rows (bank-quarters): {len(df_in):,}\",\n",
    "        f\"Unique banks: {df_in[ID_COLUMN].nunique():,}\",\n",
    "        f\"Unique quarters: {df_in[QUARTER_COLUMN].nunique():,}\",\n",
    "        f\"Feature columns: {len(feature_cols):,}\",\n",
    "        \"\",\n",
    "        \"TRANSFORMATION\",\n",
    "        \"-\" * 40,\n",
    "        \"For each bank, sorted by quarter date:\",\n",
    "        \"  change = (current - previous) / |previous| * 100\",\n",
    "        \"\",\n",
    "        f\"Banks removed (fewer than {MIN_QUARTERS_PER_BANK} quarters): {banks_removed:,}\",\n",
    "        f\"Winsorization: Values capped at {WINSORIZE_PERCENTILE}th percentile\",\n",
    "        \"\",\n",
    "        \"EDGE CASE HANDLING (Divide-by-Zero Fix)\",\n",
    "        \"-\" * 40,\n",
    "        f\"Total cells computed: {edge_case_stats['total_cells']:,}\",\n",
    "        f\"Normal computations: {edge_case_stats['normal_computation']:,}\",\n",
    "        f\"Both zero (0->0 = 0%): {edge_case_stats['both_zero']:,}\",\n",
    "        f\"From zero positive (0->+ = +{MAX_CHANGE_FROM_ZERO}%): {edge_case_stats['from_zero_positive']:,}\",\n",
    "        f\"From zero negative (0->- = -{MAX_CHANGE_FROM_ZERO}%): {edge_case_stats['from_zero_negative']:,}\",\n",
    "        f\"Previous NaN (first quarter): {edge_case_stats['previous_nan']:,}\",\n",
    "        \"\",\n",
    "        \"OUTPUT SUMMARY\",\n",
    "        \"-\" * 40,\n",
    "        f\"Output file: {OUTPUT_FILE}\",\n",
    "        f\"Rows (bank-quarters with valid changes): {len(df_out):,}\",\n",
    "        f\"Unique banks: {df_out[ID_COLUMN].nunique():,}\",\n",
    "        f\"Change columns: {len(change_cols):,}\",\n",
    "        \"\",\n",
    "        \"NOTES\",\n",
    "        \"-\" * 40,\n",
    "        \"- First quarter for each bank has NaN changes (no previous quarter)\",\n",
    "        \"- Rows with all-NaN changes are removed\",\n",
    "        \"- Original feature values are NOT included (only changes)\",\n",
    "        \"- Use IDRSSD + quarter to join back to original data if needed\",\n",
    "        \"- v0.2 FIX: Divide-by-zero cases now handled properly\",\n",
    "        \"\",\n",
    "        \"SAMPLE CHANGE COLUMNS\",\n",
    "        \"-\" * 40,\n",
    "    ]\n",
    "    \n",
    "    for col in change_cols[:20]:\n",
    "        lines.append(f\"  {col}\")\n",
    "    if len(change_cols) > 20:\n",
    "        lines.append(f\"  ... and {len(change_cols) - 20} more\")\n",
    "    \n",
    "    lines.extend([\n",
    "        \"\",\n",
    "        \"SAMPLE STATISTICS (after winsorization)\",\n",
    "        \"-\" * 40,\n",
    "    ])\n",
    "    \n",
    "    sample_cols = change_cols[:5]\n",
    "    for col in sample_cols:\n",
    "        if col in df_out.columns:\n",
    "            stats = df_out[col].describe()\n",
    "            null_count = df_out[col].isna().sum()\n",
    "            lines.append(f\"\\n{col}:\")\n",
    "            lines.append(f\"  Mean:   {stats['mean']:,.2f}%\")\n",
    "            lines.append(f\"  Std:    {stats['std']:,.2f}%\")\n",
    "            lines.append(f\"  Min:    {stats['min']:,.2f}%\")\n",
    "            lines.append(f\"  Max:    {stats['max']:,.2f}%\")\n",
    "            lines.append(f\"  NaN:    {null_count:,}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main QoQ computation pipeline (vectorized for speed).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FFIEC QUARTER-OVER-QUARTER CHANGE COMPUTATION (v0.2 - FIXED)\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(f\"Loading {INPUT_FILE}...\")\n",
    "    df = pd.read_csv(INPUT_FILE, low_memory=False)\n",
    "    print(f\"  Loaded: {df.shape[0]:,} rows x {df.shape[1]:,} columns\")\n",
    "    print(f\"  Banks: {df[ID_COLUMN].nunique():,}\")\n",
    "    print(f\"  Quarters: {df[QUARTER_COLUMN].nunique():,}\")\n",
    "    \n",
    "    # Identify feature columns (everything except ID, quarter, and other metadata)\n",
    "    exclude_cols = [ID_COLUMN, QUARTER_COLUMN] + EXCLUDE_COLUMNS\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    print(f\"  Feature columns: {len(feature_cols):,}\")\n",
    "    \n",
    "    # Force all feature columns to numeric\n",
    "    print(\"\\nConverting feature columns to numeric...\")\n",
    "    for col in feature_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Step 2: Parse quarter strings to dates\n",
    "    print(\"\\nParsing quarter dates for sorting...\")\n",
    "    df['quarter_date'] = df[QUARTER_COLUMN].apply(parse_quarter_to_date)\n",
    "    \n",
    "    # Show quarter order\n",
    "    quarter_order = df.groupby(QUARTER_COLUMN)['quarter_date'].first().sort_values()\n",
    "    print(\"  Quarter order (chronological):\")\n",
    "    for q, d in quarter_order.items():\n",
    "        date_str = d.strftime('%Y-%m-%d') if isinstance(d, datetime) else str(d)\n",
    "        print(f\"    {date_str}: {q}\")\n",
    "    \n",
    "    # Step 3: Remove banks with insufficient quarters\n",
    "    print(f\"\\nRemoving banks with fewer than {MIN_QUARTERS_PER_BANK} quarters...\")\n",
    "    quarters_per_bank = df.groupby(ID_COLUMN).size()\n",
    "    banks_to_keep = quarters_per_bank[quarters_per_bank >= MIN_QUARTERS_PER_BANK].index\n",
    "    banks_removed = df[ID_COLUMN].nunique() - len(banks_to_keep)\n",
    "    \n",
    "    df = df[df[ID_COLUMN].isin(banks_to_keep)].copy()\n",
    "    print(f\"  Banks removed: {banks_removed:,}\")\n",
    "    print(f\"  Banks remaining: {len(banks_to_keep):,}\")\n",
    "    \n",
    "    # Step 4: Filter features to only those with data in ALL quarters\n",
    "    # This ensures consistent coverage across the full time period\n",
    "    print(\"\\nFiltering features to those available in all quarters...\")\n",
    "    n_quarters = df[QUARTER_COLUMN].nunique()\n",
    "    \n",
    "    features_to_keep = []\n",
    "    features_removed = 0\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        # Count how many quarters have at least one non-null value for this feature\n",
    "        quarters_with_data = df.groupby(QUARTER_COLUMN)[col].apply(lambda x: x.notna().any()).sum()\n",
    "        \n",
    "        if quarters_with_data == n_quarters:\n",
    "            features_to_keep.append(col)\n",
    "        else:\n",
    "            features_removed += 1\n",
    "    \n",
    "    print(f\"  Features with data in all {n_quarters} quarters: {len(features_to_keep):,}\")\n",
    "    print(f\"  Features removed (incomplete coverage): {features_removed:,}\")\n",
    "    \n",
    "    feature_cols = features_to_keep\n",
    "    \n",
    "    # Step 5: Sort by bank and quarter date (required for correct shift)\n",
    "    print(\"\\nSorting data by bank and quarter...\")\n",
    "    df = df.sort_values([ID_COLUMN, 'quarter_date']).reset_index(drop=True)\n",
    "    \n",
    "    # Step 6: Compute QoQ changes (vectorized) - NOW WITH PROPER EDGE CASE HANDLING\n",
    "    print(\"\\nComputing quarter-over-quarter changes (with divide-by-zero fix)...\")\n",
    "    \n",
    "    # Get current values\n",
    "    current = df[feature_cols].copy()\n",
    "    \n",
    "    # Get previous values (shift within each bank group)\n",
    "    previous = df.groupby(ID_COLUMN)[feature_cols].shift(1)\n",
    "    \n",
    "    # Use our safe computation function\n",
    "    pct_change, edge_case_stats = compute_qoq_changes_safe(current, previous)\n",
    "    \n",
    "    # Print edge case statistics\n",
    "    print(f\"  Edge case handling:\")\n",
    "    print(f\"    - Both zero (0%): {edge_case_stats['both_zero']:,} cells\")\n",
    "    print(f\"    - From zero (+{MAX_CHANGE_FROM_ZERO}%): {edge_case_stats['from_zero_positive']:,} cells\")\n",
    "    print(f\"    - From zero (-{MAX_CHANGE_FROM_ZERO}%): {edge_case_stats['from_zero_negative']:,} cells\")\n",
    "    \n",
    "    # Rename columns to indicate QoQ\n",
    "    pct_change.columns = [col + '_qoq' for col in feature_cols]\n",
    "    change_cols = pct_change.columns.tolist()\n",
    "    \n",
    "    # Build output dataframe\n",
    "    df_changes = pd.concat([\n",
    "        df[[ID_COLUMN, QUARTER_COLUMN]].reset_index(drop=True),\n",
    "        pct_change.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Step 7: Remove rows that are all NaN (first quarter for each bank)\n",
    "    print(\"\\nRemoving rows with no valid changes (first quarter per bank)...\")\n",
    "    rows_before = len(df_changes)\n",
    "    df_changes = df_changes.dropna(subset=change_cols, how='all')\n",
    "    rows_removed = rows_before - len(df_changes)\n",
    "    print(f\"  Rows removed: {rows_removed:,}\")\n",
    "    print(f\"  Rows remaining: {len(df_changes):,}\")\n",
    "    \n",
    "    # Step 8: Winsorize extreme values\n",
    "    print(f\"\\nWinsorizing extreme values at {WINSORIZE_PERCENTILE}th percentile...\")\n",
    "    for col in change_cols:\n",
    "        lower = df_changes[col].quantile((100 - WINSORIZE_PERCENTILE) / 100)\n",
    "        upper = df_changes[col].quantile(WINSORIZE_PERCENTILE / 100)\n",
    "        df_changes[col] = df_changes[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Step 9: Reorder columns\n",
    "    output_cols = [ID_COLUMN, QUARTER_COLUMN] + sorted(change_cols)\n",
    "    df_out = df_changes[output_cols]\n",
    "    \n",
    "    # Step 10: Save outputs\n",
    "    print(f\"\\nSaving QoQ changes to {OUTPUT_FILE}...\")\n",
    "    df_out.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"  Shape: {df_out.shape[0]:,} rows x {df_out.shape[1]:,} columns\")\n",
    "    \n",
    "    # Check for remaining NaN values\n",
    "    total_nan = df_out[change_cols].isna().sum().sum()\n",
    "    total_cells = len(df_out) * len(change_cols)\n",
    "    print(f\"  Remaining NaN values: {total_nan:,} / {total_cells:,} ({total_nan/total_cells:.2%})\")\n",
    "    \n",
    "    # Generate and save report\n",
    "    print(f\"Saving report to {REPORT_FILE}...\")\n",
    "    report = generate_report(df, df_out, feature_cols, change_cols, banks_removed, edge_case_stats)\n",
    "    with open(REPORT_FILE, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Input:  {INPUT_FILE}\")\n",
    "    print(f\"Output: {OUTPUT_FILE}\")\n",
    "    print(f\"  Rows: {len(df_out):,} bank-quarter observations\")\n",
    "    print(f\"  Columns: {len(change_cols):,} QoQ change features\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Show sample statistics\n",
    "    print(\"SAMPLE QoQ CHANGE STATISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in change_cols[:5]:\n",
    "        mean = df_out[col].mean()\n",
    "        std = df_out[col].std()\n",
    "        nan_count = df_out[col].isna().sum()\n",
    "        print(f\"  {col}: mean={mean:+.2f}%, std={std:.2f}%, NaN={nan_count}\")\n",
    "    \n",
    "    return df_out, change_cols\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_qoq, change_cols = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bfb42b-3132-4e3d-8cac-d5fac2010c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
