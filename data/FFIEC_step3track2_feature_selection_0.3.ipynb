{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6481feb1-3608-44e1-8038-83229b5f6054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FFIEC FEATURE FILTERING PIPELINE (Step 3 Track 2 - Per-Bank v0.2)\n",
      "  Output format: TRANSPOSED (rows=features, columns=quarters)\n",
      "======================================================================\n",
      "\n",
      "Found 6 per-bank files in per_bank/\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Bank of America\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Loading ffiec_bank_of_america.csv...\n",
      "    Loaded: 99 rows x 6,444 columns\n",
      "    Metadata: 11 | Numeric: 5,731 | Non-numeric: 702\n",
      "    Null filter:      462 kept, 5,269 removed\n",
      "    Variance filter:  433 kept, 29 removed\n",
      "    Unique filter:    430 kept, 3 removed\n",
      "    Duplicate filter: 430 kept, 0 removed\n",
      "    => 430 features + 11 metadata = 441 columns\n",
      "    Sample quarter values: ['03/31/2001', '03/31/2002', '03/31/2003']\n",
      "    Quarter parsing: 99/99 parsed successfully\n",
      "    Transposed: 430 features x 99 quarters\n",
      "    Quarter range: 03/31/2001 -> 09/30/2025\n",
      "    Verify columns: ['03/31/2001', '06/30/2001', '09/30/2001'] ... ['03/31/2025', '06/30/2025', '09/30/2025']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Citibank\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Loading ffiec_citibank.csv...\n",
      "    Loaded: 99 rows x 6,444 columns\n",
      "    Metadata: 11 | Numeric: 5,745 | Non-numeric: 688\n",
      "    Null filter:      457 kept, 5,288 removed\n",
      "    Variance filter:  431 kept, 26 removed\n",
      "    Unique filter:    421 kept, 10 removed\n",
      "    Duplicate filter: 421 kept, 0 removed\n",
      "    => 421 features + 11 metadata = 432 columns\n",
      "    Sample quarter values: ['03/31/2001', '03/31/2002', '03/31/2003']\n",
      "    Quarter parsing: 99/99 parsed successfully\n",
      "    Transposed: 421 features x 99 quarters\n",
      "    Quarter range: 03/31/2001 -> 09/30/2025\n",
      "    Verify columns: ['03/31/2001', '06/30/2001', '09/30/2001'] ... ['03/31/2025', '06/30/2025', '09/30/2025']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Goldman Sachs Bank USA\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Loading ffiec_goldman_sachs_bank_usa.csv...\n",
      "    Loaded: 68 rows x 6,444 columns\n",
      "    Metadata: 11 | Numeric: 5,731 | Non-numeric: 702\n",
      "    Null filter:      300 kept, 5,431 removed\n",
      "    Variance filter:  217 kept, 83 removed\n",
      "    Unique filter:    189 kept, 28 removed\n",
      "    Duplicate filter: 189 kept, 0 removed\n",
      "    => 189 features + 11 metadata = 200 columns\n",
      "    Sample quarter values: ['03/31/2009', '03/31/2010', '03/31/2011']\n",
      "    Quarter parsing: 68/68 parsed successfully\n",
      "    Transposed: 189 features x 68 quarters\n",
      "    Quarter range: 12/31/2008 -> 09/30/2025\n",
      "    Verify columns: ['12/31/2008', '03/31/2009', '06/30/2009'] ... ['03/31/2025', '06/30/2025', '09/30/2025']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  JPMorgan Chase Bank\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Loading ffiec_jpmorgan_chase_bank.csv...\n",
      "    Loaded: 99 rows x 6,444 columns\n",
      "    Metadata: 11 | Numeric: 5,736 | Non-numeric: 697\n",
      "    Null filter:      448 kept, 5,288 removed\n",
      "    Variance filter:  417 kept, 31 removed\n",
      "    Unique filter:    404 kept, 13 removed\n",
      "    Duplicate filter: 404 kept, 0 removed\n",
      "    => 404 features + 11 metadata = 415 columns\n",
      "    Sample quarter values: ['03/31/2001', '03/31/2002', '03/31/2003']\n",
      "    Quarter parsing: 99/99 parsed successfully\n",
      "    Transposed: 404 features x 99 quarters\n",
      "    Quarter range: 03/31/2001 -> 09/30/2025\n",
      "    Verify columns: ['03/31/2001', '06/30/2001', '09/30/2001'] ... ['03/31/2025', '06/30/2025', '09/30/2025']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Morgan Stanley Bank\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Loading ffiec_morgan_stanley_bank.csv...\n",
      "    Loaded: 99 rows x 6,444 columns\n",
      "    Metadata: 11 | Numeric: 5,715 | Non-numeric: 718\n",
      "    Null filter:      162 kept, 5,553 removed\n",
      "    Variance filter:  105 kept, 57 removed\n",
      "    Unique filter:    87 kept, 18 removed\n",
      "    Duplicate filter: 87 kept, 0 removed\n",
      "    => 87 features + 11 metadata = 98 columns\n",
      "    Sample quarter values: ['03/31/2001', '03/31/2002', '03/31/2003']\n",
      "    Quarter parsing: 99/99 parsed successfully\n",
      "    Transposed: 87 features x 99 quarters\n",
      "    Quarter range: 03/31/2001 -> 09/30/2025\n",
      "    Verify columns: ['03/31/2001', '06/30/2001', '09/30/2001'] ... ['03/31/2025', '06/30/2025', '09/30/2025']\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Wells Fargo Bank\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Loading ffiec_wells_fargo_bank.csv...\n",
      "    Loaded: 99 rows x 6,444 columns\n",
      "    Metadata: 11 | Numeric: 5,741 | Non-numeric: 692\n",
      "    Null filter:      460 kept, 5,281 removed\n",
      "    Variance filter:  418 kept, 42 removed\n",
      "    Unique filter:    409 kept, 9 removed\n",
      "    Duplicate filter: 409 kept, 0 removed\n",
      "    => 409 features + 11 metadata = 420 columns\n",
      "    Sample quarter values: ['03/31/2001', '03/31/2002', '03/31/2003']\n",
      "    Quarter parsing: 99/99 parsed successfully\n",
      "    Transposed: 409 features x 99 quarters\n",
      "    Quarter range: 03/31/2001 -> 09/30/2025\n",
      "    Verify columns: ['03/31/2001', '06/30/2001', '09/30/2001'] ... ['03/31/2025', '06/30/2025', '09/30/2025']\n",
      "\n",
      "\n",
      "======================================================================\n",
      "CROSS-BANK FEATURE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Features common to ALL banks: 83\n",
      "Features in ANY bank:         509\n",
      "Bank-specific features:       426\n",
      "\n",
      "Per-bank summary:\n",
      "  Bank of America                |   99 quarters |  430 features | 347 unique to this bank\n",
      "  Citibank                       |   99 quarters |  421 features | 338 unique to this bank\n",
      "  Goldman Sachs Bank USA         |   68 quarters |  189 features | 106 unique to this bank\n",
      "  JPMorgan Chase Bank            |   99 quarters |  404 features | 321 unique to this bank\n",
      "  Morgan Stanley Bank            |   99 quarters |   87 features |   4 unique to this bank\n",
      "  Wells Fargo Bank               |   99 quarters |  409 features | 326 unique to this bank\n",
      "\n",
      "======================================================================\n",
      "COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Outputs saved to C:\\Users\\olive\\per_bank_features/\n",
      "  Per-bank feature CSVs:  6  (transposed: rows=features, cols=quarters)\n",
      "  Per-bank reports:       6\n",
      "  Cross-bank comparison:  feature_comparison.txt\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FFIEC Call Report Feature Filtering Script (Step 3 Track 2 - v0.2)\n",
    "===================================================================\n",
    "Purpose: Take individual per-bank CSVs from Step 2 Track 2 and filter\n",
    "         each bank down to a clean set of numeric features suitable\n",
    "         for per-bank anomaly detection / ML.\n",
    "\n",
    "Approach: Same data-driven filtering as Step 3 (null %, variance,\n",
    "          unique ratio, duplicates), but applied independently to\n",
    "          each bank. This means each bank may end up with a DIFFERENT\n",
    "          set of surviving features, which is appropriate for per-bank\n",
    "          ML models.\n",
    "\n",
    "OUTPUT FORMAT (v0.2 change): The output CSVs are TRANSPOSED so that:\n",
    "  - ROWS    = feature names (MDRM codes / field names)\n",
    "  - COLUMNS = quarters (e.g., '03/31/2000', '06/30/2000', ...)\n",
    "  This orientation makes it easy to read each feature's time series\n",
    "  across quarters horizontally.\n",
    "\n",
    "Input:  per_bank/ffiec_<bank_name>.csv  (from Step 2 Track 2)\n",
    "Output: per_bank_features/ffiec_<bank_name>_features.csv  (transposed, one per bank)\n",
    "        per_bank_features/filtering_report_<bank_name>.txt\n",
    "        per_bank_features/feature_comparison.txt  (cross-bank summary)\n",
    "\n",
    "Author: Wake Forest MSBA Practicum Team 4\n",
    "Date: February 2026\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "INPUT_DIR = Path(\"per_bank\")\n",
    "OUTPUT_DIR = Path(\"per_bank_features\")\n",
    "\n",
    "# Filtering thresholds (same as original Step 3)\n",
    "NULL_THRESHOLD = 0.0\n",
    "VARIANCE_THRESHOLD = 0.001\n",
    "UNIQUE_RATIO_THRESHOLD = 0.01\n",
    "\n",
    "# Metadata columns to keep but not treat as features\n",
    "METADATA_COLUMNS = [\n",
    "    'IDRSSD',\n",
    "    'quarter',\n",
    "    'FDIC Certificate Number',\n",
    "    'OCC Charter Number',\n",
    "    'OTS Docket Number',\n",
    "    'Primary ABA Routing Number',\n",
    "    'Financial Institution Name',\n",
    "    'Financial Institution Address',\n",
    "    'Financial Institution City',\n",
    "    'Financial Institution State',\n",
    "    'Financial Institution Zip Code',\n",
    "]\n",
    "\n",
    "# Bank name mapping (for display)\n",
    "BANKS = {\n",
    "    '480228': 'Bank of America',\n",
    "    '852218': 'JPMorgan Chase Bank',\n",
    "    '476810': 'Citibank',\n",
    "    '451965': 'Wells Fargo Bank',\n",
    "    '2182786': 'Goldman Sachs Bank USA',\n",
    "    '1456501': 'Morgan Stanley Bank',\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS (identical to Step 3 v0.3)\n",
    "# =============================================================================\n",
    "\n",
    "def identify_column_types(df):\n",
    "    \"\"\"Categorize columns into metadata, numeric, and non-numeric.\"\"\"\n",
    "    all_cols = set(df.columns)\n",
    "    metadata = set(METADATA_COLUMNS) & all_cols\n",
    "    \n",
    "    numeric_cols = []\n",
    "    non_numeric_cols = []\n",
    "    \n",
    "    for col in all_cols - metadata:\n",
    "        if col.startswith('Unnamed:'):\n",
    "            non_numeric_cols.append(col)\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            numeric_cols.append(col)\n",
    "        else:\n",
    "            try:\n",
    "                converted = pd.to_numeric(df[col], errors='coerce')\n",
    "                if converted.notna().mean() > 0.5:\n",
    "                    numeric_cols.append(col)\n",
    "                else:\n",
    "                    non_numeric_cols.append(col)\n",
    "            except:\n",
    "                non_numeric_cols.append(col)\n",
    "    \n",
    "    return {\n",
    "        'metadata': list(metadata),\n",
    "        'numeric': numeric_cols,\n",
    "        'non_numeric': non_numeric_cols\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_by_null_percentage(df, columns, threshold):\n",
    "    \"\"\"Remove columns with too many null values.\"\"\"\n",
    "    null_pct = df[columns].isnull().mean()\n",
    "    kept = null_pct[null_pct <= threshold].index.tolist()\n",
    "    removed = null_pct[null_pct > threshold]\n",
    "    removed_with_reasons = {\n",
    "        col: f\"Null percentage: {pct:.1%}\"\n",
    "        for col, pct in removed.items()\n",
    "    }\n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def filter_by_variance(df, columns, threshold):\n",
    "    \"\"\"Remove columns with near-zero variance.\"\"\"\n",
    "    kept = []\n",
    "    removed_with_reasons = {}\n",
    "    \n",
    "    for col in columns:\n",
    "        series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if series.notna().sum() == 0:\n",
    "            removed_with_reasons[col] = \"All values null after numeric conversion\"\n",
    "            continue\n",
    "        std = series.std()\n",
    "        if std == 0 or pd.isna(std):\n",
    "            removed_with_reasons[col] = f\"Zero variance (constant value: {series.dropna().iloc[0] if series.notna().any() else 'N/A'})\"\n",
    "        else:\n",
    "            normalized = (series - series.mean()) / std\n",
    "            var = normalized.var()\n",
    "            if var < threshold:\n",
    "                removed_with_reasons[col] = f\"Near-zero variance: {var:.6f}\"\n",
    "            else:\n",
    "                kept.append(col)\n",
    "    \n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def filter_by_unique_ratio(df, columns, threshold):\n",
    "    \"\"\"Remove columns with very few unique values relative to dataset size.\"\"\"\n",
    "    kept = []\n",
    "    removed_with_reasons = {}\n",
    "    n_rows = len(df)\n",
    "    \n",
    "    for col in columns:\n",
    "        series = pd.to_numeric(df[col], errors='coerce').dropna()\n",
    "        if len(series) == 0:\n",
    "            removed_with_reasons[col] = \"No valid numeric values\"\n",
    "            continue\n",
    "        n_unique = series.nunique()\n",
    "        ratio = n_unique / n_rows\n",
    "        if n_unique <= 3:\n",
    "            removed_with_reasons[col] = f\"Only {n_unique} unique values (likely categorical/flag)\"\n",
    "        elif ratio < threshold:\n",
    "            removed_with_reasons[col] = f\"Low unique ratio: {n_unique} unique / {n_rows} rows = {ratio:.6f}\"\n",
    "        else:\n",
    "            kept.append(col)\n",
    "    \n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def filter_duplicate_columns(df, columns):\n",
    "    \"\"\"Remove _dup columns that are duplicates.\"\"\"\n",
    "    kept = []\n",
    "    removed_with_reasons = {}\n",
    "    seen_bases = set()\n",
    "    \n",
    "    for col in columns:\n",
    "        dup_match = re.match(r'(.+)_dup\\d+$', col)\n",
    "        if dup_match:\n",
    "            base = dup_match.group(1)\n",
    "            if base in seen_bases:\n",
    "                removed_with_reasons[col] = f\"Duplicate of {base}\"\n",
    "            else:\n",
    "                kept.append(col)\n",
    "                seen_bases.add(base)\n",
    "        else:\n",
    "            kept.append(col)\n",
    "            seen_bases.add(col)\n",
    "    \n",
    "    return kept, removed_with_reasons\n",
    "\n",
    "\n",
    "def convert_to_numeric(df, columns):\n",
    "    \"\"\"Convert selected columns to numeric dtype.\"\"\"\n",
    "    df_out = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df_out.columns:\n",
    "            df_out[col] = pd.to_numeric(df_out[col], errors='coerce')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def generate_report(bank_name, original_count, col_types, filter_results, final_columns):\n",
    "    \"\"\"Generate a per-bank filtering report.\"\"\"\n",
    "    lines = [\n",
    "        \"=\" * 70,\n",
    "        f\"FFIEC FEATURE FILTERING REPORT - {bank_name}\",\n",
    "        f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"=\" * 70,\n",
    "        \"\",\n",
    "        \"SUMMARY\",\n",
    "        \"-\" * 40,\n",
    "        f\"Original columns:           {original_count:,}\",\n",
    "        f\"Metadata columns (kept):    {len(col_types['metadata']):,}\",\n",
    "        f\"Non-numeric (removed):      {len(col_types['non_numeric']):,}\",\n",
    "        f\"Numeric columns analyzed:   {len(col_types['numeric']):,}\",\n",
    "        f\"Final feature columns:      {len(final_columns):,}\",\n",
    "        \"\",\n",
    "        \"OUTPUT FORMAT: Transposed (rows=features, columns=quarters)\",\n",
    "        \"\",\n",
    "        \"FILTERING STEPS\",\n",
    "        \"-\" * 40,\n",
    "    ]\n",
    "    \n",
    "    for step_name, (kept_count, removed_dict) in filter_results.items():\n",
    "        lines.append(f\"\\n{step_name}:\")\n",
    "        lines.append(f\"  Kept: {kept_count:,} columns\")\n",
    "        lines.append(f\"  Removed: {len(removed_dict):,} columns\")\n",
    "        if removed_dict:\n",
    "            lines.append(\"  Sample of removed columns:\")\n",
    "            for i, (col, reason) in enumerate(list(removed_dict.items())[:10]):\n",
    "                lines.append(f\"    - {col}: {reason}\")\n",
    "            if len(removed_dict) > 10:\n",
    "                lines.append(f\"    ... and {len(removed_dict) - 10} more\")\n",
    "    \n",
    "    lines.extend([\"\", \"FINAL FEATURE COLUMNS\", \"-\" * 40, f\"Total: {len(final_columns)} columns\", \"\"])\n",
    "    \n",
    "    prefixes = {}\n",
    "    for col in sorted(final_columns):\n",
    "        prefix = col[:4] if len(col) >= 4 else col\n",
    "        if prefix not in prefixes:\n",
    "            prefixes[prefix] = []\n",
    "        prefixes[prefix].append(col)\n",
    "    for prefix in sorted(prefixes.keys()):\n",
    "        cols = prefixes[prefix]\n",
    "        lines.append(f\"{prefix}*: {len(cols)} columns\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PER-BANK FILTERING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def filter_single_bank(filepath, bank_name):\n",
    "    \"\"\"\n",
    "    Run the full feature-filtering pipeline on a single bank's CSV.\n",
    "    Returns (output_df, feature_columns, filter_summary_dict).\n",
    "    \"\"\"\n",
    "    print(f\"\\n  Loading {filepath.name}...\")\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    print(f\"    Loaded: {df.shape[0]:,} rows x {df.shape[1]:,} columns\")\n",
    "    original_count = len(df.columns)\n",
    "    \n",
    "    # Categorize columns\n",
    "    col_types = identify_column_types(df)\n",
    "    print(f\"    Metadata: {len(col_types['metadata']):,} | Numeric: {len(col_types['numeric']):,} | Non-numeric: {len(col_types['non_numeric']):,}\")\n",
    "    \n",
    "    filter_results = {}\n",
    "    current_columns = col_types['numeric']\n",
    "    \n",
    "    # Filter by null percentage\n",
    "    kept, removed = filter_by_null_percentage(df, current_columns, NULL_THRESHOLD)\n",
    "    filter_results['Null Percentage Filter'] = (len(kept), removed)\n",
    "    print(f\"    Null filter:      {len(kept):,} kept, {len(removed):,} removed\")\n",
    "    current_columns = kept\n",
    "    \n",
    "    # Filter by variance\n",
    "    kept, removed = filter_by_variance(df, current_columns, VARIANCE_THRESHOLD)\n",
    "    filter_results['Variance Filter'] = (len(kept), removed)\n",
    "    print(f\"    Variance filter:  {len(kept):,} kept, {len(removed):,} removed\")\n",
    "    current_columns = kept\n",
    "    \n",
    "    # Filter by unique ratio\n",
    "    kept, removed = filter_by_unique_ratio(df, current_columns, UNIQUE_RATIO_THRESHOLD)\n",
    "    filter_results['Unique Ratio Filter'] = (len(kept), removed)\n",
    "    print(f\"    Unique filter:    {len(kept):,} kept, {len(removed):,} removed\")\n",
    "    current_columns = kept\n",
    "    \n",
    "    # Filter duplicates\n",
    "    kept, removed = filter_duplicate_columns(df, current_columns)\n",
    "    filter_results['Duplicate Filter'] = (len(kept), removed)\n",
    "    print(f\"    Duplicate filter: {len(kept):,} kept, {len(removed):,} removed\")\n",
    "    final_feature_columns = kept\n",
    "    \n",
    "    # Build output\n",
    "    output_columns = col_types['metadata'] + final_feature_columns\n",
    "    output_columns = [c for c in output_columns if c in df.columns]\n",
    "    df_out = df[output_columns].copy()\n",
    "    df_out = convert_to_numeric(df_out, final_feature_columns)\n",
    "    \n",
    "    print(f\"    => {len(final_feature_columns):,} features + {len(col_types['metadata']):,} metadata = {len(output_columns):,} columns\")\n",
    "    \n",
    "    # Generate report\n",
    "    report = generate_report(bank_name, original_count, col_types, filter_results, final_feature_columns)\n",
    "    \n",
    "    return df_out, final_feature_columns, report\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSPOSE: Features as rows, Quarters as columns  (v0.2 addition)\n",
    "# =============================================================================\n",
    "\n",
    "def parse_quarter_to_date(quarter_str):\n",
    "    \"\"\"\n",
    "    Convert FFIEC quarter string to a sortable datetime.\n",
    "    Handles multiple possible formats:\n",
    "      - 'FFIEC CDR Call Bulk All Schedules 03312024'  (MMDDYYYY at end)\n",
    "      - '03312024' (just the date portion)\n",
    "      - '2024-03-31' (ISO format)\n",
    "      - '03/31/2024' (US date format)\n",
    "    \"\"\"\n",
    "    s = str(quarter_str).strip()\n",
    "    \n",
    "    # Try 1: Extract 8-digit date from end of string (MMDDYYYY)\n",
    "    match = re.search(r'(\\d{8})$', s)\n",
    "    if match:\n",
    "        ds = match.group(1)\n",
    "        try:\n",
    "            return datetime(int(ds[4:8]), int(ds[0:2]), int(ds[2:4]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Try 2: Extract 8-digit date from anywhere in string\n",
    "    match = re.search(r'(\\d{8})', s)\n",
    "    if match:\n",
    "        ds = match.group(1)\n",
    "        try:\n",
    "            return datetime(int(ds[4:8]), int(ds[0:2]), int(ds[2:4]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Try 3: Common date formats\n",
    "    for fmt in ('%Y-%m-%d', '%m/%d/%Y', '%m-%d-%Y', '%Y-%m-%d %H:%M:%S'):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    # Try 4: Let pandas try\n",
    "    try:\n",
    "        result = pd.to_datetime(s)\n",
    "        if pd.notna(result):\n",
    "            return result.to_pydatetime()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def transpose_bank_df(df, feature_columns, quarter_column='quarter'):\n",
    "    \"\"\"\n",
    "    Transpose a bank DataFrame so that:\n",
    "      - Rows    = feature names  (index column named 'feature')\n",
    "      - Columns = quarter labels (sorted chronologically, formatted MM/DD/YYYY)\n",
    "\n",
    "    Metadata columns are dropped; only numeric feature data is kept.\n",
    "\n",
    "    Returns:\n",
    "      df_transposed: DataFrame with features as rows and quarters as columns\n",
    "      quarter_labels: list of quarter column headers in chronological order\n",
    "    \"\"\"\n",
    "    df_work = df.copy()\n",
    "\n",
    "    # Debug: show what quarter values look like\n",
    "    sample_quarters = df_work[quarter_column].head(3).tolist()\n",
    "    print(f\"    Sample quarter values: {sample_quarters}\")\n",
    "\n",
    "    # Parse quarter strings into dates for sorting\n",
    "    df_work['_qdate'] = pd.to_datetime(df_work[quarter_column].apply(parse_quarter_to_date))\n",
    "    \n",
    "    # Debug: check how many parsed successfully\n",
    "    n_parsed = df_work['_qdate'].notna().sum()\n",
    "    n_total = len(df_work)\n",
    "    print(f\"    Quarter parsing: {n_parsed}/{n_total} parsed successfully\")\n",
    "    \n",
    "    # Drop rows where quarter couldn't be parsed\n",
    "    if n_parsed < n_total:\n",
    "        print(f\"    WARNING: Dropping {n_total - n_parsed} rows with unparseable quarters\")\n",
    "        df_work = df_work[df_work['_qdate'].notna()].copy()\n",
    "    \n",
    "    if len(df_work) == 0:\n",
    "        raise ValueError(\"No quarter values could be parsed. Check the 'quarter' column format.\")\n",
    "    \n",
    "    df_work = df_work.sort_values('_qdate').reset_index(drop=True)\n",
    "\n",
    "    # Build clean quarter labels: MM/DD/YYYY\n",
    "    quarter_labels = df_work['_qdate'].dt.strftime('%m/%d/%Y').tolist()\n",
    "    \n",
    "    # Verify no NaN labels\n",
    "    assert all(isinstance(q, str) and q != 'NaT' for q in quarter_labels), \\\n",
    "        f\"Quarter label conversion failed. Sample: {quarter_labels[:3]}\"\n",
    "\n",
    "    # Extract only feature columns, transpose\n",
    "    feature_data = df_work[feature_columns].copy()\n",
    "    \n",
    "    # Set the index to quarter labels BEFORE transposing\n",
    "    # so that after .T, the columns will be the quarter labels\n",
    "    feature_data.index = quarter_labels\n",
    "    feature_data.index.name = 'quarter'\n",
    "    \n",
    "    # Transpose: rows become columns and vice versa\n",
    "    df_transposed = feature_data.T\n",
    "    df_transposed.index.name = 'feature'\n",
    "    \n",
    "    # Verify columns are quarter strings, not integers\n",
    "    assert isinstance(df_transposed.columns[0], str), \\\n",
    "        f\"Expected string column names, got {type(df_transposed.columns[0])}: {df_transposed.columns[0]}\"\n",
    "\n",
    "    return df_transposed, quarter_labels\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"FFIEC FEATURE FILTERING PIPELINE (Step 3 Track 2 - Per-Bank v0.2)\")\n",
    "    print(\"  Output format: TRANSPOSED (rows=features, columns=quarters)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Discover per-bank CSVs\n",
    "    bank_files = sorted(INPUT_DIR.glob(\"ffiec_*.csv\"))\n",
    "    # Exclude the combined file if present\n",
    "    bank_files = [f for f in bank_files if 'filtered_banks' not in f.name]\n",
    "    \n",
    "    if not bank_files:\n",
    "        print(f\"\\nERROR: No per-bank CSV files found in {INPUT_DIR}/\")\n",
    "        print(\"  Run Step 2 Track 2 first.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nFound {len(bank_files)} per-bank files in {INPUT_DIR}/\")\n",
    "    \n",
    "    # Process each bank\n",
    "    all_results = {}  # bank_name -> {features, n_rows, n_features, filepath}\n",
    "    \n",
    "    for filepath in bank_files:\n",
    "        # Derive bank name from filename\n",
    "        # e.g., ffiec_bank_of_america.csv -> bank_of_america\n",
    "        slug = filepath.stem.replace('ffiec_', '')\n",
    "        \n",
    "        # Find the matching display name\n",
    "        bank_name = slug.replace('_', ' ').title()\n",
    "        for rssd, name in BANKS.items():\n",
    "            if name.lower().replace(' ', '_').replace('.', '').replace(',', '') == slug:\n",
    "                bank_name = name\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"  {bank_name}\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        \n",
    "        df_out, feature_cols, report = filter_single_bank(filepath, bank_name)\n",
    "        \n",
    "        # ---- v0.2: Transpose before saving ----\n",
    "        df_transposed, quarter_labels = transpose_bank_df(df_out, feature_cols)\n",
    "        n_features = len(feature_cols)\n",
    "        n_quarters = len(quarter_labels)\n",
    "        print(f\"    Transposed: {n_features:,} features x {n_quarters} quarters\")\n",
    "        print(f\"    Quarter range: {quarter_labels[0]} -> {quarter_labels[-1]}\")\n",
    "        print(f\"    Verify columns: {list(df_transposed.columns[:3])} ... {list(df_transposed.columns[-3:])}\")\n",
    "        \n",
    "        # Save transposed output\n",
    "        out_csv = OUTPUT_DIR / f\"ffiec_{slug}_features.csv\"\n",
    "        out_report = OUTPUT_DIR / f\"filtering_report_{slug}.txt\"\n",
    "        \n",
    "        df_transposed.to_csv(out_csv)  # index=True is default, writes 'feature' as first col\n",
    "        with open(out_report, 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        all_results[bank_name] = {\n",
    "            'features': set(feature_cols),\n",
    "            'n_quarters': n_quarters,\n",
    "            'n_features': n_features,\n",
    "            'filepath': out_csv.name,\n",
    "        }\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Cross-bank comparison\n",
    "    # =========================================================================\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(\"CROSS-BANK FEATURE COMPARISON\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Features common to ALL banks\n",
    "    all_feature_sets = [info['features'] for info in all_results.values()]\n",
    "    common_features = set.intersection(*all_feature_sets) if all_feature_sets else set()\n",
    "    union_features = set.union(*all_feature_sets) if all_feature_sets else set()\n",
    "    \n",
    "    print(f\"Features common to ALL banks: {len(common_features):,}\")\n",
    "    print(f\"Features in ANY bank:         {len(union_features):,}\")\n",
    "    print(f\"Bank-specific features:       {len(union_features) - len(common_features):,}\")\n",
    "    \n",
    "    print(f\"\\nPer-bank summary:\")\n",
    "    for bank_name, info in all_results.items():\n",
    "        unique_count = len(info['features'] - common_features)\n",
    "        print(f\"  {bank_name:30s} | {info['n_quarters']:>4} quarters | {info['n_features']:>4} features | {unique_count:>3} unique to this bank\")\n",
    "    \n",
    "    # Save cross-bank comparison report\n",
    "    comparison_lines = [\n",
    "        \"=\" * 70,\n",
    "        \"CROSS-BANK FEATURE COMPARISON\",\n",
    "        f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"=\" * 70,\n",
    "        \"\",\n",
    "        \"Output format: TRANSPOSED (rows=features, columns=quarters)\",\n",
    "        \"\",\n",
    "        f\"Features common to ALL banks: {len(common_features)}\",\n",
    "        f\"Features in ANY bank: {len(union_features)}\",\n",
    "        \"\",\n",
    "        \"PER-BANK SUMMARY\",\n",
    "        \"-\" * 40,\n",
    "    ]\n",
    "    for bank_name, info in all_results.items():\n",
    "        comparison_lines.append(f\"{bank_name}: {info['n_features']} features, {info['n_quarters']} quarters\")\n",
    "    \n",
    "    comparison_lines.extend([\"\", \"COMMON FEATURES (in all banks)\", \"-\" * 40])\n",
    "    for col in sorted(common_features):\n",
    "        comparison_lines.append(f\"  {col}\")\n",
    "    \n",
    "    comparison_lines.extend([\"\", \"BANK-SPECIFIC FEATURES\", \"-\" * 40])\n",
    "    for bank_name, info in all_results.items():\n",
    "        unique = sorted(info['features'] - common_features)\n",
    "        if unique:\n",
    "            comparison_lines.append(f\"\\n{bank_name} only ({len(unique)}):\")\n",
    "            for col in unique:\n",
    "                comparison_lines.append(f\"  {col}\")\n",
    "    \n",
    "    with open(OUTPUT_DIR / \"feature_comparison.txt\", 'w') as f:\n",
    "        f.write(\"\\n\".join(comparison_lines))\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nOutputs saved to {OUTPUT_DIR.resolve()}/\")\n",
    "    print(f\"  Per-bank feature CSVs:  {len(all_results)}  (transposed: rows=features, cols=quarters)\")\n",
    "    print(f\"  Per-bank reports:       {len(all_results)}\")\n",
    "    print(f\"  Cross-bank comparison:  feature_comparison.txt\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb9ddb-be51-454c-b877-31acdb8fd319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
